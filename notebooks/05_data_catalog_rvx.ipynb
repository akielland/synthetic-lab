{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d1e47ef",
   "metadata": {},
   "source": [
    "# RVX Data Catalog & Exploration\n",
    "\n",
    "Comprehensive exploration of all datasets in the RVX folders:\n",
    "- `traveling_survey/` - National travel survey data\n",
    "- `zonal_register_data/` - Zonal statistical data (SDAT files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82197e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/anderskielland/Documents/Synthetic data/code/synthetic-lab\n",
      "RVX path: /Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/data/raw/population/rvx\n",
      "Path exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# Setup\n",
    "from synlab.utils import get_project_root\n",
    "\n",
    "project_root = get_project_root()\n",
    "rvx_path = project_root / 'data' / 'raw' / 'population' / 'rvx'\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"RVX path: {rvx_path}\")\n",
    "print(f\"Path exists: {rvx_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd9cb88",
   "metadata": {},
   "source": [
    "## 1. Folder Overview\n",
    "\n",
    "List all files and their sizes in both folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e045c1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìÅ TRAVELING_SURVEY\n",
      "================================================================================\n",
      "\n",
      "Total files: 8\n",
      "\n",
      "File listing:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Filemail.com - Nasjonal RVU akkumulert data.zip                  228.44 MB  .zip\n",
      "Filemail.com - RVU 2025.zip                                      128.01 MB  .zip\n",
      "Nasjonal_RVU_PERSON_Nov26_0901.sav                                63.65 MB  .sav\n",
      "Nasjonal_RVU_REISER_Nov26_0901.sav                                64.36 MB  .sav\n",
      "Oppdatert skjema RVU_2025.docx                                     0.37 MB  .docx\n",
      "RVU 2019-2024 Personfil Vektet 251125.sav                         93.56 MB  .sav\n",
      "RVU 2019_2024 Reisefil 251107.sav                                134.88 MB  .sav\n",
      "Sp√∏rreskjema_RVU_2021_2024.docx                                    0.20 MB  .docx\n",
      "\n",
      "By extension:\n",
      "  .docx             2 files\n",
      "  .sav              4 files\n",
      "  .zip              2 files\n",
      "\n",
      "================================================================================\n",
      "üìÅ ZONAL_REGISTER_DATA\n",
      "================================================================================\n",
      "\n",
      "Total files: 28\n",
      "\n",
      "File listing:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "sdat1_d2024_g2020.dbf                                              4.42 MB  .dbf\n",
      "sdat1_d2024_g2021.dbf                                              4.42 MB  .dbf\n",
      "sdat1_d2024_g2023.dbf                                              4.43 MB  .dbf\n",
      "sdat1_d2024_g2024.dbf                                              4.43 MB  .dbf\n",
      "sdat2_data2020_delomr.xlsx                                         3.42 MB  .xlsx\n",
      "sdat2_data2020_grunnkrets.xlsx                                    25.69 MB  .xlsx\n",
      "sdat3_d2023x_g2020.dbf                                             2.38 MB  .dbf\n",
      "sdat3_d2023x_g2021.dbf                                             2.38 MB  .dbf\n",
      "sdat3_d2023x_g2023.dbf                                             2.38 MB  .dbf\n",
      "sdat3_d2023x_g2024.dbf                                             2.38 MB  .dbf\n",
      "sdat4_d2024_g2020.dbf                                              2.60 MB  .dbf\n",
      "sdat4_d2024_g2021.dbf                                              2.60 MB  .dbf\n",
      "sdat4_d2024_g2023.dbf                                              2.60 MB  .dbf\n",
      "sdat4_d2024_g2024.dbf                                              2.60 MB  .dbf\n",
      "sdat5_d2023_g2020.dbf                                              0.55 MB  .dbf\n",
      "sdat5_d2023_g2021.dbf                                              0.55 MB  .dbf\n",
      "sdat5_d2023_g2023.dbf                                              0.55 MB  .dbf\n",
      "sdat5_d2023_g2024.dbf                                              0.55 MB  .dbf\n",
      "sdat71_NB2023_grk2020_2020.dbf                                     0.39 MB  .dbf\n",
      "sdat7_d20xx_g2020_ikke_pkost.dbf                                   0.66 MB  .dbf\n",
      "sdat7_d20xx_g2021_ikke_pkost.dbf                                   0.66 MB  .dbf\n",
      "sdat7_d20xx_g2023_ikke_pkost.dbf                                   0.66 MB  .dbf\n",
      "sdat7_d20xx_g2024_ikke_pkost.dbf                                   0.66 MB  .dbf\n",
      "sdat8_d2024_g2020.dbf                                              0.87 MB  .dbf\n",
      "sdat8_d2024_g2021.dbf                                              0.87 MB  .dbf\n",
      "sdat8_d2024_g2023.dbf                                              0.87 MB  .dbf\n",
      "sdat8_d2024_g2024.dbf                                              0.87 MB  .dbf\n",
      "sdat_6_areal_g2020.dbf                                             3.98 MB  .dbf\n",
      "\n",
      "By extension:\n",
      "  .dbf             26 files\n",
      "  .xlsx             2 files\n"
     ]
    }
   ],
   "source": [
    "def get_folder_structure(folder_path):\n",
    "    \"\"\"\n",
    "    Walk through folder and collect all files with metadata.\n",
    "    Returns list of dicts with file info.\n",
    "    \"\"\"\n",
    "    files_info = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        # Skip .DS_Store and other system files\n",
    "        files = [f for f in files if not f.startswith('.')]\n",
    "        \n",
    "        for file in files:\n",
    "            file_path = Path(root) / file\n",
    "            size_bytes = os.path.getsize(file_path)\n",
    "            size_mb = size_bytes / (1024 * 1024)\n",
    "            \n",
    "            rel_path = file_path.relative_to(folder_path)\n",
    "            \n",
    "            files_info.append({\n",
    "                'filename': file,\n",
    "                'relative_path': str(rel_path),\n",
    "                'full_path': str(file_path),\n",
    "                'size_bytes': size_bytes,\n",
    "                'size_mb': round(size_mb, 2),\n",
    "                'extension': Path(file).suffix\n",
    "            })\n",
    "    \n",
    "    return sorted(files_info, key=lambda x: x['filename'])\n",
    "\n",
    "# Explore both folders\n",
    "folders = ['traveling_survey', 'zonal_register_data']\n",
    "all_files = {}\n",
    "\n",
    "for folder_name in folders:\n",
    "    folder_path = rvx_path / folder_name\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìÅ {folder_name.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    files = get_folder_structure(folder_path)\n",
    "    all_files[folder_name] = files\n",
    "    \n",
    "    print(f\"\\nTotal files: {len(files)}\")\n",
    "    print(f\"\\nFile listing:\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for f in files:\n",
    "        print(f\"{f['filename']:<60} {f['size_mb']:>10.2f} MB  {f['extension']}\")\n",
    "    \n",
    "    # Summary by extension\n",
    "    by_ext = defaultdict(int)\n",
    "    for f in files:\n",
    "        by_ext[f['extension']] += 1\n",
    "    \n",
    "    print(f\"\\nBy extension:\")\n",
    "    for ext, count in sorted(by_ext.items()):\n",
    "        print(f\"  {ext if ext else '[no ext]':<15} {count:>3} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64ced7e",
   "metadata": {},
   "source": [
    "## 2. Explore Data File Types\n",
    "\n",
    "Understand the structure of different data formats (.dbf, .sav, .xlsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a435442d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì pyreadstat already installed\n",
      "‚úì openpyxl already installed\n",
      "‚úì dbfread already installed\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries for reading different formats\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = ['pyreadstat', 'openpyxl', 'dbfread']\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"‚úì {package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
    "        print(f\"‚úì {package} installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6120731a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìä DBF FILES (zonal_register_data)\n",
      "================================================================================\n",
      "\n",
      "Total DBF files: 26\n",
      "‚úì DBF schema extraction complete\n"
     ]
    }
   ],
   "source": [
    "# Try reading different file types\n",
    "import pyreadstat\n",
    "from dbfread import DBF\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "file_schemas = {}\n",
    "\n",
    "# 1. DBF files (zonal_register_data)\n",
    "print(\"=\"*80)\n",
    "print(\"üìä DBF FILES (zonal_register_data)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "dbf_files = [f for f in all_files['zonal_register_data'] if f['extension'] == '.dbf']\n",
    "print(f\"\\nTotal DBF files: {len(dbf_files)}\")\n",
    "\n",
    "for dbf_file in dbf_files:\n",
    "    key = dbf_file['relative_path']\n",
    "    sample_path = dbf_file['full_path']\n",
    "    \n",
    "    try:\n",
    "        table = DBF(sample_path, encoding='latin-1')\n",
    "        columns_detail = [\n",
    "            {\n",
    "                'name': field.name,\n",
    "                'type': field.type,\n",
    "                'length': field.length,\n",
    "                'decimals': field.decimal_count\n",
    "            }\n",
    "            for field in table.fields\n",
    "        ]\n",
    "        \n",
    "        rows = len(table)\n",
    "        file_schemas[key] = {\n",
    "            'file_type': 'DBF',\n",
    "            'rows': rows,\n",
    "            'columns': len(columns_detail),\n",
    "            'columns_detail': columns_detail\n",
    "        }\n",
    "    except Exception as e:\n",
    "        file_schemas[key] = {\n",
    "            'file_type': 'DBF',\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "print(\"‚úì DBF schema extraction complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eed3d519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä SPSS SAV FILES (traveling_survey)\n",
      "================================================================================\n",
      "\n",
      "Total SAV files: 4\n",
      "‚úì SAV schema extraction complete\n"
     ]
    }
   ],
   "source": [
    "# 2. SPSS/SAV files (traveling_survey)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä SPSS SAV FILES (traveling_survey)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sav_files = [f for f in all_files['traveling_survey'] if f['extension'].lower() == '.sav']\n",
    "print(f\"\\nTotal SAV files: {len(sav_files)}\")\n",
    "\n",
    "for sav_file in sav_files:\n",
    "    key = sav_file['relative_path']\n",
    "    try:\n",
    "        try:\n",
    "            df, meta = pyreadstat.read_sav(sav_file['full_path'])\n",
    "            rows = meta.number_rows if hasattr(meta, 'number_rows') else len(df)\n",
    "        except TypeError:\n",
    "            df, meta = pyreadstat.read_sav(sav_file['full_path'])\n",
    "            rows = len(df)\n",
    "        \n",
    "        columns_detail = [\n",
    "            {'name': col, 'type': str(df[col].dtype)}\n",
    "            for col in df.columns\n",
    "        ]\n",
    "        \n",
    "        file_schemas[key] = {\n",
    "            'file_type': 'SAV (SPSS)',\n",
    "            'rows': rows,\n",
    "            'columns': len(df.columns),\n",
    "            'columns_detail': columns_detail\n",
    "        }\n",
    "    except Exception as e:\n",
    "        file_schemas[key] = {\n",
    "            'file_type': 'SAV (SPSS)',\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "print(\"‚úì SAV schema extraction complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d065687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä EXCEL FILES (.xlsx)\n",
      "================================================================================\n",
      "\n",
      "Total XLSX files: 2\n",
      "‚úì XLSX schema extraction complete\n"
     ]
    }
   ],
   "source": [
    "# 3. XLSX files\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä EXCEL FILES (.xlsx)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "xlsx_files = [f for f in all_files['zonal_register_data'] if f['extension'].lower() == '.xlsx']\n",
    "print(f\"\\nTotal XLSX files: {len(xlsx_files)}\")\n",
    "\n",
    "for xlsx_file in xlsx_files:\n",
    "    key = xlsx_file['relative_path']\n",
    "    try:\n",
    "        wb = load_workbook(xlsx_file['full_path'], read_only=True, data_only=True)\n",
    "        sheets_info = []\n",
    "        \n",
    "        for sheet_name in wb.sheetnames:\n",
    "            ws = wb[sheet_name]\n",
    "            header_row = next(ws.iter_rows(min_row=1, max_row=1, values_only=True), [])\n",
    "            header = [str(h) if h is not None else '' for h in header_row]\n",
    "            \n",
    "            sheets_info.append({\n",
    "                'sheet': sheet_name,\n",
    "                'rows': ws.max_row,\n",
    "                'columns': len(header),\n",
    "                'column_names': header\n",
    "            })\n",
    "        \n",
    "        file_schemas[key] = {\n",
    "            'file_type': 'XLSX',\n",
    "            'sheets': sheets_info\n",
    "        }\n",
    "    except Exception as e:\n",
    "        file_schemas[key] = {\n",
    "            'file_type': 'XLSX',\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "print(\"‚úì XLSX schema extraction complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d22c878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üì¶ ARCHIVES\n",
      "================================================================================\n",
      "\n",
      "Total ZIP files: 2\n",
      "\n",
      "üîç Filemail.com - Nasjonal RVU akkumulert data.zip (228.44 MB)\n",
      "  Contains 2 files:\n",
      "    - RVU 2019-2024 Personfil Vektet 251125.sav             93.56 MB\n",
      "    - RVU 2019_2024 Reisefil 251107.sav                    134.88 MB\n",
      "\n",
      "üîç Filemail.com - RVU 2025.zip (128.01 MB)\n",
      "  Contains 2 files:\n",
      "    - Nasjonal_RVU_PERSON_Nov26_0901.sav                    63.65 MB\n",
      "    - Nasjonal_RVU_REISER_Nov26_0901.sav                    64.36 MB\n"
     ]
    }
   ],
   "source": [
    "# 4. ZIP and other archives\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üì¶ ARCHIVES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "zip_files = [f for f in all_files['traveling_survey'] if f['extension'].lower() == '.zip']\n",
    "print(f\"\\nTotal ZIP files: {len(zip_files)}\")\n",
    "\n",
    "import zipfile\n",
    "for zip_file in zip_files:\n",
    "    print(f\"\\nüîç {zip_file['filename']} ({zip_file['size_mb']:.2f} MB)\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file['full_path'], 'r') as z:\n",
    "            file_list = z.namelist()\n",
    "            print(f\"  Contains {len(file_list)} files:\")\n",
    "            for fname in sorted(file_list)[:10]:  # First 10\n",
    "                info = z.getinfo(fname)\n",
    "                size_mb = info.file_size / (1024*1024)\n",
    "                print(f\"    - {fname:<50} {size_mb:>8.2f} MB\")\n",
    "            if len(file_list) > 10:\n",
    "                print(f\"    ... and {len(file_list) - 10} more files\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Could not read: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de516308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìä DATA PROFILING - Analyzing column values\n",
      "================================================================================\n",
      "\n",
      "1. Profiling DBF files...\n",
      "  [1/26] sdat1_d2024_g2020.dbf... ‚úì\n",
      "  [2/26] sdat1_d2024_g2021.dbf... ‚úì\n",
      "  [3/26] sdat1_d2024_g2023.dbf... ‚úì\n",
      "  [4/26] sdat1_d2024_g2024.dbf... ‚úì\n",
      "  [5/26] sdat3_d2023x_g2020.dbf... ‚úì\n",
      "  [6/26] sdat3_d2023x_g2021.dbf... ‚úì\n",
      "  [7/26] sdat3_d2023x_g2023.dbf... ‚úì\n",
      "  [8/26] sdat3_d2023x_g2024.dbf... ‚úì\n",
      "  [9/26] sdat4_d2024_g2020.dbf... ‚úì\n",
      "  [10/26] sdat4_d2024_g2021.dbf... ‚úì\n",
      "  [11/26] sdat4_d2024_g2023.dbf... ‚úì\n",
      "  [12/26] sdat4_d2024_g2024.dbf... ‚úì\n",
      "  [13/26] sdat5_d2023_g2020.dbf... ‚úì\n",
      "  [14/26] sdat5_d2023_g2021.dbf... ‚úì\n",
      "  [15/26] sdat5_d2023_g2023.dbf... ‚úì\n",
      "  [16/26] sdat5_d2023_g2024.dbf... ‚úì\n",
      "  [17/26] sdat71_NB2023_grk2020_2020.dbf... ‚úì\n",
      "  [18/26] sdat7_d20xx_g2020_ikke_pkost.dbf... ‚úì\n",
      "  [19/26] sdat7_d20xx_g2021_ikke_pkost.dbf... ‚úì\n",
      "  [20/26] sdat7_d20xx_g2023_ikke_pkost.dbf... ‚úì\n",
      "  [21/26] sdat7_d20xx_g2024_ikke_pkost.dbf... ‚úì\n",
      "  [22/26] sdat8_d2024_g2020.dbf... ‚úì\n",
      "  [23/26] sdat8_d2024_g2021.dbf... ‚úì\n",
      "  [24/26] sdat8_d2024_g2023.dbf... ‚úì\n",
      "  [25/26] sdat8_d2024_g2024.dbf... ‚úì\n",
      "  [26/26] sdat_6_areal_g2020.dbf... ‚úì\n",
      "\n",
      "2. Profiling SAV files...\n",
      "  [1/4] Nasjonal_RVU_PERSON_Nov26_0901.sav... ‚úì\n",
      "  [2/4] Nasjonal_RVU_REISER_Nov26_0901.sav... ‚úì\n",
      "  [4/4] RVU 2019_2024 Reisefil 251107.sav... ‚úì\n",
      "\n",
      "‚úÖ Data profiling complete!\n",
      "   DBF files profiled: 26/26\n",
      "   SAV files profiled: 3/4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä DATA PROFILING - Analyzing column values\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def profile_column(series):\n",
    "    \"\"\"Profile a single column (numeric or categorical)\"\"\"\n",
    "    profile = {}\n",
    "    \n",
    "    # Basic info\n",
    "    total_count = len(series)\n",
    "    null_count = series.isna().sum()\n",
    "    profile['null_count'] = int(null_count)\n",
    "    profile['null_pct'] = round(100 * null_count / total_count, 1) if total_count > 0 else 0\n",
    "    \n",
    "    # Skip if all nulls\n",
    "    if null_count == total_count:\n",
    "        profile['type'] = 'empty'\n",
    "        return profile\n",
    "    \n",
    "    # Check if numeric\n",
    "    if pd.api.types.is_numeric_dtype(series):\n",
    "        profile['type'] = 'numeric'\n",
    "        clean = series.dropna()\n",
    "        profile['min'] = float(clean.min())\n",
    "        profile['max'] = float(clean.max())\n",
    "        profile['mean'] = round(float(clean.mean()), 2)\n",
    "        profile['median'] = float(clean.median())\n",
    "        profile['unique'] = int(series.nunique())\n",
    "    else:\n",
    "        # Categorical\n",
    "        profile['type'] = 'categorical'\n",
    "        profile['unique'] = int(series.nunique())\n",
    "        \n",
    "        # Top 5 values with counts\n",
    "        value_counts = series.value_counts().head(5)\n",
    "        profile['top_values'] = [\n",
    "            {'value': str(val), 'count': int(count), 'pct': round(100 * count / total_count, 1)}\n",
    "            for val, count in value_counts.items()\n",
    "        ]\n",
    "    \n",
    "    return profile\n",
    "\n",
    "# Profile DBF files\n",
    "print(\"\\n1. Profiling DBF files...\")\n",
    "dbf_files = [f for f in all_files['zonal_register_data'] if f['extension'] == '.dbf']\n",
    "\n",
    "for idx, dbf_file in enumerate(dbf_files):\n",
    "    key = dbf_file['relative_path']\n",
    "    \n",
    "    if key not in file_schemas or 'error' in file_schemas[key]:\n",
    "        continue\n",
    "    \n",
    "    print(f\"  [{idx+1}/{len(dbf_files)}] {dbf_file['filename'][:50]}...\", end=\" \")\n",
    "    \n",
    "    try:\n",
    "        # Read full file\n",
    "        table = DBF(dbf_file['full_path'], encoding='latin-1')\n",
    "        df = pd.DataFrame(list(table))\n",
    "        \n",
    "        # Profile each column\n",
    "        column_profiles = {}\n",
    "        for col in df.columns:\n",
    "            column_profiles[col] = profile_column(df[col])\n",
    "        \n",
    "        file_schemas[key]['column_profiles'] = column_profiles\n",
    "        print(\"‚úì\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error: {str(e)[:50]}\")\n",
    "        file_schemas[key]['profile_error'] = str(e)\n",
    "\n",
    "# Profile SAV files\n",
    "print(\"\\n2. Profiling SAV files...\")\n",
    "sav_files = [f for f in all_files['traveling_survey'] if f['extension'].lower() == '.sav']\n",
    "\n",
    "for idx, sav_file in enumerate(sav_files):\n",
    "    key = sav_file['relative_path']\n",
    "    \n",
    "    if key not in file_schemas or 'error' in file_schemas[key]:\n",
    "        continue\n",
    "    \n",
    "    print(f\"  [{idx+1}/{len(sav_files)}] {sav_file['filename'][:50]}...\", end=\" \")\n",
    "    \n",
    "    try:\n",
    "        # Read full file\n",
    "        df, meta = pyreadstat.read_sav(sav_file['full_path'])\n",
    "        \n",
    "        # Profile each column\n",
    "        column_profiles = {}\n",
    "        for col in df.columns:\n",
    "            column_profiles[col] = profile_column(df[col])\n",
    "        \n",
    "        file_schemas[key]['column_profiles'] = column_profiles\n",
    "        print(\"‚úì\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error: {str(e)[:50]}\")\n",
    "        file_schemas[key]['profile_error'] = str(e)\n",
    "\n",
    "print(\"\\n‚úÖ Data profiling complete!\")\n",
    "print(f\"   DBF files profiled: {sum(1 for f in dbf_files if 'column_profiles' in file_schemas.get(f['relative_path'], {}))}/{len(dbf_files)}\")\n",
    "print(f\"   SAV files profiled: {sum(1 for f in sav_files if 'column_profiles' in file_schemas.get(f['relative_path'], {}))}/{len(sav_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137d8d84",
   "metadata": {},
   "source": [
    "## 2.5 Data Profiling\n",
    "\n",
    "Profile each column to extract:\n",
    "- **Numeric columns:** min, max, mean, median\n",
    "- **Categorical columns:** unique count, top 5 most frequent values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afafbb43",
   "metadata": {},
   "source": [
    "## 3. Generate Data Catalog Markdown\n",
    "\n",
    "Create a comprehensive markdown document of all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4de4b51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated catalog (first 2000 chars):\n",
      "# RVX Data Catalog\n",
      "\n",
      "**Generated:** 2026-02-04 11:58:18\n",
      "\n",
      "Complete inventory and documentation of all datasets in the RVX folders.\n",
      "\n",
      "## Overview\n",
      "\n",
      "The RVX folder contains two main data sources:\n",
      "\n",
      "1. **traveling_survey/** - National travel survey (RVU - Reisevaneunders√∏kelsen)\n",
      "2. **zonal_register_data/** - Zonal statistical data (SDAT files) from Statistics Norway\n",
      "\n",
      "## Folder Structure\n",
      "\n",
      "\n",
      "### traveling_survey/\n",
      "\n",
      "**National Travel Survey Data (RVU)**\n",
      "\n",
      "Contains survey responses about travel behavior of Norwegian households.\n",
      "- **Format:** SPSS (.sav), ZIP archives, documentation (.docx)\n",
      "- **Source:** Statistics Norway (SSB)\n",
      "- **Coverage:** Years 2019-2025\n",
      "\n",
      "**Statistics:**\n",
      "- Total files: 8\n",
      "- Total size: 713.47 MB\n",
      "\n",
      "**Files:**\n",
      "\n",
      "| Filename | Size (MB) | Type | Rows | Columns |\n",
      "|----------|-----------|------|------|---------|\n",
      "| `Filemail.com - Nasjonal RVU akkumulert data.zip` | 228.44 | .zip | n/a | n/a |\n",
      "| `Filemail.com - RVU 2025.zip` | 128.01 | .zip | n/a | n/a |\n",
      "| `Filemail.com - RVU 2025/Nasjonal_RVU_PERSON_Nov26_0901.sav` | 63.65 | .sav | 51330 | 453 |\n",
      "| `Filemail.com - RVU 2025/Nasjonal_RVU_REISER_Nov26_0901.sav` | 64.36 | .sav | 127115 | 226 |\n",
      "| `Oppdatert skjema RVU_2025.docx` | 0.37 | .docx | n/a | n/a |\n",
      "| `Filemail.com - Nasjonal RVU akkumulert data/RVU 2019-2024 Personfil Vektet 251125.sav` | 93.56 | .sav | n/a | n/a |\n",
      "| `Filemail.com - Nasjonal RVU akkumulert data/RVU 2019_2024 Reisefil 251107.sav` | 134.88 | .sav | 597747 | 103 |\n",
      "| `Sp√∏rreskjema_RVU_2021_2024.docx` | 0.20 | .docx | n/a | n/a |\n",
      "\n",
      "\n",
      "### zonal_register_data/\n",
      "\n",
      "**Zonal Statistical Data (SDAT)**\n",
      "\n",
      "Grid-based statistical data at different geographic resolutions.\n",
      "- **Format:** DBF (dBase), XLSX\n",
      "- **Source:** Statistics Norway (TRAMOD/RVX)\n",
      "- **Coverage:** Multiple grid resolutions (grunnkrets, delomr, etc.)\n",
      "- **Data years:** 2020-2024\n",
      "\n",
      "**Statistics:**\n",
      "- Total files: 28\n",
      "- Total size: 79.42 MB\n",
      "\n",
      "**Files:**\n",
      "\n",
      "| Filename | Size (MB) | Type | Rows | Columns |\n",
      "|----------|-----------|------|------|---------|\n",
      "| `sdat1_* (\n"
     ]
    }
   ],
   "source": [
    "# Generate comprehensive markdown catalog\n",
    "import re\n",
    "\n",
    "catalog_md = f\"\"\"# RVX Data Catalog\n",
    "\n",
    "**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "Complete inventory and documentation of all datasets in the RVX folders.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The RVX folder contains two main data sources:\n",
    "\n",
    "1. **traveling_survey/** - National travel survey (RVU - Reisevaneunders√∏kelsen)\n",
    "2. **zonal_register_data/** - Zonal statistical data (SDAT files) from Statistics Norway\n",
    "\n",
    "## Folder Structure\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def group_sdat_files(files):\n",
    "    sdat_files = [\n",
    "        f for f in files\n",
    "        if f['filename'].lower().startswith('sdat') and f['extension'] == '.dbf'\n",
    "    ]\n",
    "    other_files = [f for f in files if f not in sdat_files]\n",
    "\n",
    "    groups = {}\n",
    "    for f in sdat_files:\n",
    "        name = f['filename'].lower()\n",
    "        match = re.match(r\"^(sdat\\d+)_\", name)\n",
    "        if not match:\n",
    "            match = re.match(r\"^(sdat_\\d+)_\", name)\n",
    "        if match:\n",
    "            key = match.group(1)\n",
    "        else:\n",
    "            key = name.split('.')[0]\n",
    "        groups.setdefault(key, []).append(f)\n",
    "\n",
    "    return groups, other_files\n",
    "\n",
    "def group_xlsx_files(files):\n",
    "    \"\"\"Group XLSX files by prefix (e.g., sdat2_data2020_*)\"\"\"\n",
    "    xlsx_files = [\n",
    "        f for f in files\n",
    "        if f['extension'].lower() == '.xlsx'\n",
    "    ]\n",
    "    other_files = [f for f in files if f not in xlsx_files]\n",
    "    \n",
    "    groups = {}\n",
    "    for f in xlsx_files:\n",
    "        name = f['filename'].lower()\n",
    "        # Extract prefix like \"sdat2_data2020\"\n",
    "        match = re.match(r\"^(sdat\\d+_data\\d+)_\", name)\n",
    "        if match:\n",
    "            key = match.group(1)\n",
    "        else:\n",
    "            key = name.split('_')[0] if '_' in name else name.split('.')[0]\n",
    "        groups.setdefault(key, []).append(f)\n",
    "    \n",
    "    return groups, other_files\n",
    "\n",
    "def get_demographic_breakdown():\n",
    "    \"\"\"Returns documentation about the demographic structure in XLSX files\"\"\"\n",
    "    return \"\"\"\n",
    "**Column Structure & Demographics:**\n",
    "\n",
    "The 360+ columns in each geographic area are structured as:\n",
    "**24 demographic cells** (age √ó gender) √ó **15 household categories** = **360 variables**\n",
    "\n",
    "**Age & Gender (24 cells):**\n",
    "- 12 age groups √ó 2 genders = 24 combinations\n",
    "\n",
    "**Household Categories (15 categories):**\n",
    "- Number of adults: 1 adult / 2 adults / 3+ adults\n",
    "- Household type:\n",
    "  - Enslig uten barn (Single without children)\n",
    "  - Enslig med barn (Single with children)\n",
    "  - Par uten barn (Couple without children)\n",
    "  - Par med barn (Couple with children)\n",
    "  - Flere voksne (Multiple adults)\n",
    "\n",
    "**Calculation:** 3 adult categories √ó 5 household types = 15 categories per age/gender group\n",
    "\"\"\"\n",
    "\n",
    "def summarize_group(group_files):\n",
    "    sizes = sum(f['size_mb'] for f in group_files)\n",
    "    schemas = [file_schemas.get(f['relative_path'], {}) for f in group_files]\n",
    "\n",
    "    rows_list = [s.get('rows') for s in schemas if isinstance(s.get('rows'), int)]\n",
    "    cols_list = [s.get('columns') for s in schemas if isinstance(s.get('columns'), int)]\n",
    "\n",
    "    if rows_list:\n",
    "        rows = f\"{min(rows_list)}‚Äì{max(rows_list)}\" if min(rows_list) != max(rows_list) else f\"{rows_list[0]}\"\n",
    "    else:\n",
    "        rows = \"n/a\"\n",
    "\n",
    "    if cols_list:\n",
    "        cols = f\"{cols_list[0]}\" if len(set(cols_list)) == 1 else f\"{min(cols_list)}‚Äì{max(cols_list)}\"\n",
    "    else:\n",
    "        cols = \"n/a\"\n",
    "\n",
    "    return sizes, rows, cols\n",
    "\n",
    "# Add folder summaries\n",
    "for folder_name in folders:\n",
    "    files = all_files[folder_name]\n",
    "    total_size = sum(f['size_mb'] for f in files)\n",
    "\n",
    "    catalog_md += f\"\\n### {folder_name}/\\n\\n\"\n",
    "\n",
    "    if folder_name == 'traveling_survey':\n",
    "        catalog_md += \"\"\"**National Travel Survey Data (RVU)**\n",
    "\n",
    "Contains survey responses about travel behavior of Norwegian households.\n",
    "- **Format:** SPSS (.sav), ZIP archives, documentation (.docx)\n",
    "- **Source:** Statistics Norway (SSB)\n",
    "- **Coverage:** Years 2019-2025\n",
    "\n",
    "\"\"\"\n",
    "    else:\n",
    "        catalog_md += \"\"\"**Zonal Statistical Data (SDAT)**\n",
    "\n",
    "Grid-based statistical data at different geographic resolutions.\n",
    "- **Format:** DBF (dBase), XLSX\n",
    "- **Source:** Statistics Norway (TRAMOD/RVX)\n",
    "- **Coverage:** Multiple grid resolutions (grunnkrets, delomr, etc.)\n",
    "- **Data years:** 2020-2024\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    catalog_md += f\"**Statistics:**\\n\"\n",
    "    catalog_md += f\"- Total files: {len(files)}\\n\"\n",
    "    catalog_md += f\"- Total size: {total_size:.2f} MB\\n\\n\"\n",
    "\n",
    "    # List files\n",
    "    catalog_md += \"**Files:**\\n\\n\"\n",
    "    catalog_md += \"| Filename | Size (MB) | Type | Rows | Columns |\\n\"\n",
    "    catalog_md += \"|----------|-----------|------|------|---------|\\n\"\n",
    "\n",
    "    if folder_name == 'zonal_register_data':\n",
    "        groups, other_files = group_sdat_files(files)\n",
    "\n",
    "        for group_key in sorted(groups.keys()):\n",
    "            group_files = sorted(groups[group_key], key=lambda x: x['filename'])\n",
    "            size_mb, rows, cols = summarize_group(group_files)\n",
    "            label = f\"{group_key}_* ({len(group_files)} files)\"\n",
    "            catalog_md += f\"| `{label}` | {size_mb:.2f} | .dbf | {rows} | {cols} |\\n\"\n",
    "\n",
    "        for f in sorted(other_files, key=lambda x: x['filename']):\n",
    "            schema = file_schemas.get(f['relative_path'], {})\n",
    "            rows = schema.get('rows', 'n/a')\n",
    "            cols = schema.get('columns', 'n/a')\n",
    "            catalog_md += f\"| `{f['relative_path']}` | {f['size_mb']:.2f} | {f['extension'] or 'dir'} | {rows} | {cols} |\\n\"\n",
    "    else:\n",
    "        for f in files:\n",
    "            schema = file_schemas.get(f['relative_path'], {})\n",
    "            rows = schema.get('rows', 'n/a')\n",
    "            cols = schema.get('columns', 'n/a')\n",
    "            catalog_md += f\"| `{f['relative_path']}` | {f['size_mb']:.2f} | {f['extension'] or 'dir'} | {rows} | {cols} |\\n\"\n",
    "\n",
    "    catalog_md += \"\\n\"\n",
    "\n",
    "# Add detailed schema section\n",
    "catalog_md += \"## Dataset Schemas\\n\\n\"\n",
    "\n",
    "for folder_name in folders:\n",
    "    catalog_md += f\"### {folder_name}/\\n\\n\"\n",
    "    files = all_files[folder_name]\n",
    "\n",
    "    if folder_name == 'zonal_register_data':\n",
    "        groups, other_files = group_sdat_files(files)\n",
    "\n",
    "        for group_key in sorted(groups.keys()):\n",
    "            group_files = sorted(groups[group_key], key=lambda x: x['filename'])\n",
    "            base_file = group_files[0]\n",
    "            base_schema = file_schemas.get(base_file['relative_path'], {})\n",
    "\n",
    "            cols_detail = base_schema.get('columns_detail', [])\n",
    "            base_cols = [c.get('name', '') for c in cols_detail]\n",
    "\n",
    "            all_same = True\n",
    "            for f in group_files[1:]:\n",
    "                schema = file_schemas.get(f['relative_path'], {})\n",
    "                cols = [c.get('name', '') for c in schema.get('columns_detail', [])]\n",
    "                if cols != base_cols:\n",
    "                    all_same = False\n",
    "                    break\n",
    "\n",
    "            size_mb, rows, cols = summarize_group(group_files)\n",
    "            file_list = \", \".join([f\"{f['filename']}\" for f in group_files])\n",
    "\n",
    "            catalog_md += f\"#### {group_key}_* ({len(group_files)} files)\\n\\n\"\n",
    "            catalog_md += f\"- **Files:** {file_list}\\n\"\n",
    "            catalog_md += f\"- **Type:** DBF\\n\"\n",
    "            catalog_md += f\"- **Rows:** {rows}\\n\"\n",
    "            catalog_md += f\"- **Columns:** {cols}\\n\\n\"\n",
    "\n",
    "            if all_same and cols_detail:\n",
    "                columns_detail = base_schema.get('columns_detail', [])\n",
    "                column_profiles = base_schema.get('column_profiles', {})\n",
    "                \n",
    "                catalog_md += \"**Column details (shared across group):**\\n\\n\"\n",
    "                if column_profiles:\n",
    "                    catalog_md += \"| Column | Type | Range/Values | Nulls |\\n\"\n",
    "                    catalog_md += \"|--------|------|--------------|-------|\\n\"\n",
    "                    for col in cols_detail:\n",
    "                        col_name = col.get('name', '')\n",
    "                        col_type = col.get('type', '')\n",
    "                        profile = column_profiles.get(col_name, {})\n",
    "                        \n",
    "                        # Format range/values\n",
    "                        if profile.get('type') == 'numeric':\n",
    "                            range_str = f\"{profile.get('min', '?')} ‚Äì {profile.get('max', '?')} (Œº={profile.get('mean', '?')})\"\n",
    "                        elif profile.get('type') == 'categorical':\n",
    "                            top = profile.get('top_values', [])[:3]\n",
    "                            if top:\n",
    "                                range_str = ', '.join([f\"{v['value']}({v['pct']}%)\" for v in top])\n",
    "                                if profile.get('unique', 0) > 3:\n",
    "                                    range_str += f\" ... ({profile['unique']} unique)\"\n",
    "                            else:\n",
    "                                range_str = f\"{profile.get('unique', '?')} unique\"\n",
    "                        else:\n",
    "                            range_str = \"‚Äî\"\n",
    "                        \n",
    "                        # Format nulls\n",
    "                        null_pct = profile.get('null_pct', 0)\n",
    "                        null_str = f\"{null_pct}%\" if null_pct > 0 else \"‚Äî\"\n",
    "                        \n",
    "                        catalog_md += f\"| {col_name} | {col_type} | {range_str} | {null_str} |\\n\"\n",
    "                else:\n",
    "                    catalog_md += \"| Column | Type |\\n\"\n",
    "                    catalog_md += \"|--------|------|\\n\"\n",
    "                    for col in cols_detail:\n",
    "                        col_name = col.get('name', '')\n",
    "                        col_type = col.get('type', '')\n",
    "                        catalog_md += f\"| {col_name} | {col_type} |\\n\"\n",
    "                catalog_md += \"\\n\"\n",
    "            else:\n",
    "                catalog_md += \"**Note:** Columns differ across files in this group.\\n\\n\"\n",
    "\n",
    "        # Non-SDAT files - group XLSX files by prefix\n",
    "        xlsx_groups, remaining_files = group_xlsx_files(other_files)\n",
    "        \n",
    "        for group_key in sorted(xlsx_groups.keys()):\n",
    "            group_files = sorted(xlsx_groups[group_key], key=lambda x: x['filename'])\n",
    "            \n",
    "            # Get columns from each file\n",
    "            group_schemas = {}\n",
    "            for f in group_files:\n",
    "                key = f['relative_path']\n",
    "                schema = file_schemas.get(key, {})\n",
    "                sheets = schema.get('sheets', [])\n",
    "                if sheets:\n",
    "                    col_names = sheets[0].get('column_names', [])\n",
    "                    group_schemas[f['filename']] = col_names\n",
    "            \n",
    "            # Check if all files have identical columns\n",
    "            base_file = group_files[0]\n",
    "            base_cols = group_schemas.get(base_file['filename'], [])\n",
    "            \n",
    "            all_same = True\n",
    "            different_cols = {}\n",
    "            for f in group_files[1:]:\n",
    "                cols = group_schemas.get(f['filename'], [])\n",
    "                if cols != base_cols:\n",
    "                    all_same = False\n",
    "                    missing = [c for c in base_cols if c not in cols]\n",
    "                    extra = [c for c in cols if c not in base_cols]\n",
    "                    different_cols[f['filename']] = {'missing': missing, 'extra': extra}\n",
    "            \n",
    "            file_list = \", \".join([f\"{f['filename']}\" for f in group_files])\n",
    "            base_schema = file_schemas.get(base_file['relative_path'], {})\n",
    "            sheets = base_schema.get('sheets', [])\n",
    "            \n",
    "            catalog_md += f\"#### {group_key}_* ({len(group_files)} files)\\n\\n\"\n",
    "            catalog_md += f\"- **Files:** {file_list}\\n\"\n",
    "            catalog_md += f\"- **Type:** XLSX\\n\"\n",
    "            \n",
    "            if sheets:\n",
    "                catalog_md += f\"- **Sheets per file:** {len(sheets)}\\n\"\n",
    "                if all_same:\n",
    "                    catalog_md += f\"- **Common columns:** {len(base_cols)}\\n\"\n",
    "                    catalog_md += f\"- **Shared schema:** ‚úÖ All files have identical column structure\\n\\n\"\n",
    "                else:\n",
    "                    catalog_md += f\"- **Shared columns:** {len(base_cols)}\\n\"\n",
    "                    catalog_md += f\"- **Schema:** Geographic-level variations\\n\\n\"\n",
    "                \n",
    "                # Add demographic breakdown explanation\n",
    "                catalog_md += get_demographic_breakdown()\n",
    "                catalog_md += \"\\n\\n\"\n",
    "                \n",
    "                # Show common columns\n",
    "                catalog_md += \"**Common columns (subset - first 10):**\\n\\n\"\n",
    "                catalog_md += \"| Column | Description |\\n\"\n",
    "                catalog_md += \"|--------|-------------|\\n\"\n",
    "                for col in base_cols[:10]:\n",
    "                    catalog_md += f\"| {col} | Demographic √ó Household |\\n\"\n",
    "                if len(base_cols) > 10:\n",
    "                    catalog_md += f\"| ... | ({len(base_cols) - 10} more demographic combinations) |\\n\"\n",
    "                catalog_md += \"\\n\"\n",
    "                \n",
    "                # If not all same, show differences\n",
    "                if not all_same:\n",
    "                    catalog_md += \"**Geographic-level differences:**\\n\\n\"\n",
    "                    for fname, diffs in different_cols.items():\n",
    "                        if diffs['extra']:\n",
    "                            catalog_md += f\"- **{fname}**: +{', '.join(diffs['extra'])}\\n\"\n",
    "                    catalog_md += \"\\n\"\n",
    "            catalog_md += \"\\n\"\n",
    "        \n",
    "        # Handle remaining non-grouped files\n",
    "        for f in sorted(remaining_files, key=lambda x: x['filename']):\n",
    "            key = f['relative_path']\n",
    "            schema = file_schemas.get(key)\n",
    "            if not schema:\n",
    "                continue\n",
    "\n",
    "            catalog_md += f\"#### {f['filename']}\\n\\n\"\n",
    "            catalog_md += f\"- **Path:** `{key}`\\n\"\n",
    "            catalog_md += f\"- **Type:** {schema.get('file_type', 'Unknown')}\\n\"\n",
    "\n",
    "            if 'error' in schema:\n",
    "                catalog_md += f\"- **Error:** {schema['error']}\\n\\n\"\n",
    "                continue\n",
    "    else:\n",
    "        for f in files:\n",
    "            key = f['relative_path']\n",
    "            schema = file_schemas.get(key)\n",
    "            if not schema:\n",
    "                continue\n",
    "\n",
    "            catalog_md += f\"#### {f['filename']}\\n\\n\"\n",
    "            catalog_md += f\"- **Path:** `{key}`\\n\"\n",
    "            catalog_md += f\"- **Type:** {schema.get('file_type', 'Unknown')}\\n\"\n",
    "\n",
    "            if 'error' in schema:\n",
    "                catalog_md += f\"- **Error:** {schema['error']}\\n\\n\"\n",
    "                continue\n",
    "\n",
    "            if schema.get('file_type') == 'XLSX':\n",
    "                catalog_md += f\"- **Sheets:** {len(schema.get('sheets', []))}\\n\\n\"\n",
    "                for sheet in schema.get('sheets', []):\n",
    "                    catalog_md += f\"  - **Sheet:** {sheet['sheet']}\\n\"\n",
    "                    catalog_md += f\"    - Rows: {sheet['rows']}\\n\"\n",
    "                    catalog_md += f\"    - Columns: {sheet['columns']}\\n\"\n",
    "                    catalog_md += f\"    - Column names: {', '.join(sheet['column_names'])}\\n\"\n",
    "                catalog_md += \"\\n\"\n",
    "            else:\n",
    "                catalog_md += f\"- **Rows:** {schema.get('rows', 'n/a')}\\n\"\n",
    "                catalog_md += f\"- **Columns:** {schema.get('columns', 'n/a')}\\n\\n\"\n",
    "\n",
    "                columns_detail = schema.get('columns_detail', [])\n",
    "                column_profiles = schema.get('column_profiles', {})\n",
    "                \n",
    "                if columns_detail:\n",
    "                    catalog_md += \"**Column details:**\\n\\n\"\n",
    "                    if column_profiles:\n",
    "                        catalog_md += \"| Column | Type | Range/Values | Nulls |\\n\"\n",
    "                        catalog_md += \"|--------|------|--------------|-------|\\n\"\n",
    "                        for col in columns_detail:\n",
    "                            col_name = col.get('name', '')\n",
    "                            col_type = col.get('type', '')\n",
    "                            profile = column_profiles.get(col_name, {})\n",
    "                            \n",
    "                            # Format range/values\n",
    "                            if profile.get('type') == 'numeric':\n",
    "                                range_str = f\"{profile.get('min', '?')} ‚Äì {profile.get('max', '?')} (Œº={profile.get('mean', '?')})\"\n",
    "                            elif profile.get('type') == 'categorical':\n",
    "                                top = profile.get('top_values', [])[:3]\n",
    "                                if top:\n",
    "                                    range_str = ', '.join([f\"{v['value']}({v['pct']}%)\" for v in top])\n",
    "                                    if profile.get('unique', 0) > 3:\n",
    "                                        range_str += f\" ... ({profile['unique']} unique)\"\n",
    "                                else:\n",
    "                                    range_str = f\"{profile.get('unique', '?')} unique\"\n",
    "                            else:\n",
    "                                range_str = \"‚Äî\"\n",
    "                            \n",
    "                            # Format nulls\n",
    "                            null_pct = profile.get('null_pct', 0)\n",
    "                            null_str = f\"{null_pct}%\" if null_pct > 0 else \"‚Äî\"\n",
    "                            \n",
    "                            catalog_md += f\"| {col_name} | {col_type} | {range_str} | {null_str} |\\n\"\n",
    "                    else:\n",
    "                        catalog_md += \"| Column | Type |\\n\"\n",
    "                        catalog_md += \"|--------|------|\\n\"\n",
    "                        for col in columns_detail:\n",
    "                            col_name = col.get('name', '')\n",
    "                            col_type = col.get('type', '')\n",
    "                            catalog_md += f\"| {col_name} | {col_type} |\\n\"\n",
    "                    catalog_md += \"\\n\"\n",
    "\n",
    "print(\"Generated catalog (first 2000 chars):\")\n",
    "print(catalog_md[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f07b935b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved catalog to: /Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/docs/DATA_CATALOG_RVX.md\n",
      "\n",
      "File size: 67.65 KB\n"
     ]
    }
   ],
   "source": [
    "# Save the catalog markdown\n",
    "catalog_path = project_root / 'docs' / 'DATA_CATALOG_RVX.md'\n",
    "catalog_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(catalog_path, 'w') as f:\n",
    "    f.write(catalog_md)\n",
    "\n",
    "print(f\"‚úì Saved catalog to: {catalog_path}\")\n",
    "print(f\"\\nFile size: {catalog_path.stat().st_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6f6a29",
   "metadata": {},
   "source": [
    "## 4. Summary\n",
    "\n",
    "Check what we've discovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8206cff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATA CATALOG SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üìÅ traveling_survey:\n",
      "   Files: 8\n",
      "   Total size: 713.47 MB\n",
      "   Types: .docx(2), .sav(4), .zip(2)\n",
      "\n",
      "üìÅ zonal_register_data:\n",
      "   Files: 28\n",
      "   Total size: 79.42 MB\n",
      "   Types: .dbf(26), .xlsx(2)\n",
      "\n",
      "‚úÖ Full catalog saved to: /Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/docs/DATA_CATALOG_RVX.md\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA CATALOG SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for folder_name in folders:\n",
    "    files = all_files[folder_name]\n",
    "    total_size = sum(f['size_mb'] for f in files)\n",
    "    \n",
    "    print(f\"\\nüìÅ {folder_name}:\")\n",
    "    print(f\"   Files: {len(files)}\")\n",
    "    print(f\"   Total size: {total_size:.2f} MB\")\n",
    "    \n",
    "    by_ext = defaultdict(int)\n",
    "    for f in files:\n",
    "        by_ext[f['extension']] += 1\n",
    "    \n",
    "    # Build type summary string\n",
    "    type_summary = ', '.join(f\"{ext or '[none]'}({c})\" for ext, c in sorted(by_ext.items()))\n",
    "    print(f\"   Types: {type_summary}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Full catalog saved to: {catalog_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc75b434",
   "metadata": {},
   "source": [
    "## 5. Create Pretty Outputs\n",
    "Convert the markdown catalog to HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b137b49",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a75526c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved styled HTML to: /Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/docs/DATA_CATALOG_RVX.html\n",
      "   Open in browser: file:///Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/docs/DATA_CATALOG_RVX.html\n",
      "   File size: 106.20 KB\n"
     ]
    }
   ],
   "source": [
    "# 2. Create styled HTML version (beautiful webpage)\n",
    "# Convert markdown to HTML first\n",
    "import markdown\n",
    "html_content = markdown.markdown(catalog_md, extensions=['tables'])\n",
    "\n",
    "# Create full HTML page with styling (using f-string to avoid format conflicts)\n",
    "full_html = f\"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>RVX Data Catalog</title>\n",
    "    <style>\n",
    "        body {{\n",
    "            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;\n",
    "            line-height: 1.6;\n",
    "            max-width: 1200px;\n",
    "            margin: 0 auto;\n",
    "            padding: 20px;\n",
    "            background-color: #f5f5f5;\n",
    "        }}\n",
    "        .container {{\n",
    "            background-color: white;\n",
    "            padding: 40px;\n",
    "            border-radius: 8px;\n",
    "            box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n",
    "        }}\n",
    "        h1 {{\n",
    "            color: #2c3e50;\n",
    "            border-bottom: 3px solid #3498db;\n",
    "            padding-bottom: 10px;\n",
    "        }}\n",
    "        h2 {{\n",
    "            color: #34495e;\n",
    "            margin-top: 30px;\n",
    "            border-bottom: 2px solid #ecf0f1;\n",
    "            padding-bottom: 8px;\n",
    "        }}\n",
    "        h3 {{\n",
    "            color: #7f8c8d;\n",
    "        }}\n",
    "        table {{\n",
    "            width: 100%;\n",
    "            border-collapse: collapse;\n",
    "            margin: 20px 0;\n",
    "            background-color: white;\n",
    "        }}\n",
    "        th {{\n",
    "            background-color: #3498db;\n",
    "            color: white;\n",
    "            padding: 12px;\n",
    "            text-align: left;\n",
    "            font-weight: 600;\n",
    "        }}\n",
    "        td {{\n",
    "            padding: 10px 12px;\n",
    "            border-bottom: 1px solid #ecf0f1;\n",
    "        }}\n",
    "        tr:hover {{\n",
    "            background-color: #f8f9fa;\n",
    "        }}\n",
    "        code {{\n",
    "            background-color: #f4f4f4;\n",
    "            padding: 2px 6px;\n",
    "            border-radius: 3px;\n",
    "            font-family: 'Courier New', monospace;\n",
    "            font-size: 0.9em;\n",
    "        }}\n",
    "        ul {{\n",
    "            padding-left: 25px;\n",
    "        }}\n",
    "        li {{\n",
    "            margin: 8px 0;\n",
    "        }}\n",
    "        .generated-date {{\n",
    "            color: #95a5a6;\n",
    "            font-style: italic;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        {html_content}\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Save HTML file\n",
    "html_path = project_root / 'docs' / 'DATA_CATALOG_RVX.html'\n",
    "with open(html_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(full_html)\n",
    "\n",
    "print(f\"‚úÖ Saved styled HTML to: {html_path}\")\n",
    "print(f\"   Open in browser: file://{html_path}\")\n",
    "print(f\"   File size: {html_path.stat().st_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3df7e50",
   "metadata": {},
   "source": [
    "## 6. Interactive Data Inspector\n",
    "\n",
    "Browse through DBF files to verify schema and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53fbeedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìã AVAILABLE DBF FILES\n",
      "================================================================================\n",
      "\n",
      "Change the file_index variable below to inspect different files:\n",
      "\n",
      " 0. sdat1_d2024_g2020.dbf                              (4.42 MB, 14097 rows)\n",
      " 1. sdat1_d2024_g2021.dbf                              (4.42 MB, 14097 rows)\n",
      " 2. sdat1_d2024_g2023.dbf                              (4.43 MB, 14101 rows)\n",
      " 3. sdat1_d2024_g2024.dbf                              (4.43 MB, 14101 rows)\n",
      " 4. sdat3_d2023x_g2020.dbf                             (2.38 MB, 14097 rows)\n",
      " 5. sdat3_d2023x_g2021.dbf                             (2.38 MB, 14097 rows)\n",
      " 6. sdat3_d2023x_g2023.dbf                             (2.38 MB, 14101 rows)\n",
      " 7. sdat3_d2023x_g2024.dbf                             (2.38 MB, 14101 rows)\n",
      " 8. sdat4_d2024_g2020.dbf                              (2.60 MB, 14097 rows)\n",
      " 9. sdat4_d2024_g2021.dbf                              (2.60 MB, 14097 rows)\n",
      "10. sdat4_d2024_g2023.dbf                              (2.60 MB, 14101 rows)\n",
      "11. sdat4_d2024_g2024.dbf                              (2.60 MB, 14101 rows)\n",
      "12. sdat5_d2023_g2020.dbf                              (0.55 MB, 14097 rows)\n",
      "13. sdat5_d2023_g2021.dbf                              (0.55 MB, 14097 rows)\n",
      "14. sdat5_d2023_g2023.dbf                              (0.55 MB, 14101 rows)\n",
      "15. sdat5_d2023_g2024.dbf                              (0.55 MB, 14101 rows)\n",
      "16. sdat71_NB2023_grk2020_2020.dbf                     (0.39 MB, 14097 rows)\n",
      "17. sdat7_d20xx_g2020_ikke_pkost.dbf                   (0.66 MB, 14097 rows)\n",
      "18. sdat7_d20xx_g2021_ikke_pkost.dbf                   (0.66 MB, 14097 rows)\n",
      "19. sdat7_d20xx_g2023_ikke_pkost.dbf                   (0.66 MB, 14101 rows)\n",
      "20. sdat7_d20xx_g2024_ikke_pkost.dbf                   (0.66 MB, 14101 rows)\n",
      "21. sdat8_d2024_g2020.dbf                              (0.87 MB, 14097 rows)\n",
      "22. sdat8_d2024_g2021.dbf                              (0.87 MB, 14097 rows)\n",
      "23. sdat8_d2024_g2023.dbf                              (0.87 MB, 14101 rows)\n",
      "24. sdat8_d2024_g2024.dbf                              (0.87 MB, 14101 rows)\n",
      "25. sdat_6_areal_g2020.dbf                             (3.98 MB, 14097 rows)\n",
      "\n",
      "Total: 26 DBF files\n"
     ]
    }
   ],
   "source": [
    "# List all available DBF files\n",
    "print(\"=\"*80)\n",
    "print(\"üìã AVAILABLE DBF FILES\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nChange the file_index variable below to inspect different files:\\n\")\n",
    "\n",
    "dbf_file_list = [f for f in all_files['zonal_register_data'] if f['extension'] == '.dbf']\n",
    "\n",
    "for idx, f in enumerate(dbf_file_list):\n",
    "    print(f\"{idx:2d}. {f['filename']:<50} ({f['size_mb']:.2f} MB, {file_schemas.get(f['relative_path'], {}).get('rows', '?')} rows)\")\n",
    "\n",
    "print(f\"\\nTotal: {len(dbf_file_list)} DBF files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4b81ffba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'sdat4_d2024_g2021.dbf', 'relative_path': 'sdat4_d2024_g2021.dbf', 'full_path': '/Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/data/raw/population/rvx/zonal_register_data/sdat4_d2024_g2021.dbf', 'size_bytes': 2721523, 'size_mb': 2.6, 'extension': '.dbf'}\n",
      "sdat4_d2024_g2021.dbf (2.60 MB)\n",
      "\n",
      "14097 rows x 24 columns\n",
      "\n",
      "Schema check (columns + DBF type):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Column</th>\n",
       "      <td>GRUNNKRETS</td>\n",
       "      <td>A10PRI</td>\n",
       "      <td>A20SEK</td>\n",
       "      <td>A21SEK</td>\n",
       "      <td>A30VH</td>\n",
       "      <td>A31VH</td>\n",
       "      <td>A32VH</td>\n",
       "      <td>A33VH</td>\n",
       "      <td>A34VH</td>\n",
       "      <td>A40TJE</td>\n",
       "      <td>...</td>\n",
       "      <td>A60UND</td>\n",
       "      <td>A61UND</td>\n",
       "      <td>A62UND</td>\n",
       "      <td>A63UND</td>\n",
       "      <td>A70HSOS</td>\n",
       "      <td>A71HSOS</td>\n",
       "      <td>A72HSOS</td>\n",
       "      <td>A73HSOS</td>\n",
       "      <td>MALINT</td>\n",
       "      <td>FEMINT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DBF Type</th>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows √ó 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0       1       2       3      4      5      6      7   \\\n",
       "Column    GRUNNKRETS  A10PRI  A20SEK  A21SEK  A30VH  A31VH  A32VH  A33VH   \n",
       "DBF Type           N       N       N       N      N      N      N      N   \n",
       "\n",
       "             8       9   ...      14      15      16      17       18  \\\n",
       "Column    A34VH  A40TJE  ...  A60UND  A61UND  A62UND  A63UND  A70HSOS   \n",
       "DBF Type      N       N  ...       N       N       N       N        N   \n",
       "\n",
       "               19       20       21      22      23  \n",
       "Column    A71HSOS  A72HSOS  A73HSOS  MALINT  FEMINT  \n",
       "DBF Type        N        N        N       N       N  \n",
       "\n",
       "[2 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRUNNKRETS</th>\n",
       "      <th>A10PRI</th>\n",
       "      <th>A20SEK</th>\n",
       "      <th>A21SEK</th>\n",
       "      <th>A30VH</th>\n",
       "      <th>A31VH</th>\n",
       "      <th>A32VH</th>\n",
       "      <th>A33VH</th>\n",
       "      <th>A34VH</th>\n",
       "      <th>A40TJE</th>\n",
       "      <th>...</th>\n",
       "      <th>A60UND</th>\n",
       "      <th>A61UND</th>\n",
       "      <th>A62UND</th>\n",
       "      <th>A63UND</th>\n",
       "      <th>A70HSOS</th>\n",
       "      <th>A71HSOS</th>\n",
       "      <th>A72HSOS</th>\n",
       "      <th>A73HSOS</th>\n",
       "      <th>MALINT</th>\n",
       "      <th>FEMINT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3010101</td>\n",
       "      <td>1.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1060.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>747.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3010102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>545.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>324.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9090.0</td>\n",
       "      <td>...</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>465.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3243.0</td>\n",
       "      <td>197.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3010103</td>\n",
       "      <td>0.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>407.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>1324.0</td>\n",
       "      <td>1023.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6369.0</td>\n",
       "      <td>...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1625.0</td>\n",
       "      <td>956.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3010104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1510.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>1494.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5046.0</td>\n",
       "      <td>...</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1521.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>542.0</td>\n",
       "      <td>693.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3010105</td>\n",
       "      <td>30.0</td>\n",
       "      <td>467.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>447.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4881.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1321.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1220.0</td>\n",
       "      <td>153.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3010201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2254.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4302.0</td>\n",
       "      <td>...</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1693.0</td>\n",
       "      <td>311.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows √ó 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   GRUNNKRETS  A10PRI  A20SEK  A21SEK  A30VH  A31VH   A32VH   A33VH  A34VH  \\\n",
       "0     3010101     1.0   165.0     0.0  123.0    4.0    34.0   315.0    0.0   \n",
       "1     3010102     0.0   545.0     0.0  238.0   73.0   324.0   489.0    5.0   \n",
       "2     3010103     0.0   372.0     0.0  407.0  112.0  1324.0  1023.0    0.0   \n",
       "3     3010104     0.0  1510.0     0.0  344.0  434.0   880.0  1494.0    1.0   \n",
       "4     3010105    30.0   467.0     0.0  161.0   25.0    36.0   447.0    6.0   \n",
       "5     3010201     0.0  2254.0     0.0  340.0   40.0    65.0   111.0    0.0   \n",
       "\n",
       "   A40TJE  ...  A60UND  A61UND  A62UND  A63UND  A70HSOS  A71HSOS  A72HSOS  \\\n",
       "0  1060.0  ...     0.0     0.0     0.0     0.0      8.0      0.0      0.0   \n",
       "1  9090.0  ...   122.0     0.0    44.0    33.0    465.0     48.0      6.0   \n",
       "2  6369.0  ...    44.0     0.0     0.0     0.0     30.0     58.0      0.0   \n",
       "3  5046.0  ...    52.0     0.0     0.0  1521.0     78.0     24.0      0.0   \n",
       "4  4881.0  ...     8.0     0.0     0.0     5.0   1321.0     41.0      3.0   \n",
       "5  4302.0  ...   104.0     0.0   182.0     0.0    495.0     25.0      1.0   \n",
       "\n",
       "   A73HSOS  MALINT  FEMINT  \n",
       "0      0.0   747.0     5.0  \n",
       "1      0.0  3243.0   197.0  \n",
       "2      0.0  1625.0   956.0  \n",
       "3      1.0   542.0   693.0  \n",
       "4     32.0  1220.0   153.0  \n",
       "5      7.0  1693.0   311.0  \n",
       "\n",
       "[6 rows x 24 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change this number to inspect different files (0 to 25)\n",
    "file_index = 9\n",
    "\n",
    "# Load and display\n",
    "selected = dbf_file_list[file_index]\n",
    "print(selected)\n",
    "table = DBF(selected['full_path'], encoding='latin-1')\n",
    "df = pd.DataFrame(list(table))\n",
    "\n",
    "print(f\"{selected['filename']} ({selected['size_mb']:.2f} MB)\\n\")\n",
    "print(f\"{df.shape[0]} rows x {df.shape[1]} columns\\n\")\n",
    "\n",
    "# Generic schema check: column names and DBF types (2-row table)\n",
    "row_names = [field.name for field in table.fields]\n",
    "row_types = [field.type for field in table.fields]\n",
    "\n",
    "schema_check = pd.DataFrame([row_names, row_types], index=[\"Column\", \"DBF Type\"])\n",
    "print(\"Schema check (columns + DBF type):\")\n",
    "display(schema_check)\n",
    "\n",
    "# Raw preview\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e6606996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'filename': 'sdat1_d2024_g2020.dbf',\n",
       "  'relative_path': 'sdat1_d2024_g2020.dbf',\n",
       "  'full_path': '/Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/data/raw/population/rvx/zonal_register_data/sdat1_d2024_g2020.dbf',\n",
       "  'size_bytes': 4639259,\n",
       "  'size_mb': 4.42,\n",
       "  'extension': '.dbf'},\n",
       " {'filename': 'sdat1_d2024_g2021.dbf',\n",
       "  'relative_path': 'sdat1_d2024_g2021.dbf',\n",
       "  'full_path': '/Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/data/raw/population/rvx/zonal_register_data/sdat1_d2024_g2021.dbf',\n",
       "  'size_bytes': 4639259,\n",
       "  'size_mb': 4.42,\n",
       "  'extension': '.dbf'},\n",
       " {'filename': 'sdat1_d2024_g2023.dbf',\n",
       "  'relative_path': 'sdat1_d2024_g2023.dbf',\n",
       "  'full_path': '/Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/data/raw/population/rvx/zonal_register_data/sdat1_d2024_g2023.dbf',\n",
       "  'size_bytes': 4640575,\n",
       "  'size_mb': 4.43,\n",
       "  'extension': '.dbf'},\n",
       " {'filename': 'sdat1_d2024_g2024.dbf',\n",
       "  'relative_path': 'sdat1_d2024_g2024.dbf',\n",
       "  'full_path': '/Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/data/raw/population/rvx/zonal_register_data/sdat1_d2024_g2024.dbf',\n",
       "  'size_bytes': 4640575,\n",
       "  'size_mb': 4.43,\n",
       "  'extension': '.dbf'},\n",
       " {'filename': 'sdat3_d2023x_g2020.dbf',\n",
       "  'relative_path': 'sdat3_d2023x_g2020.dbf',\n",
       "  'full_path': '/Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/data/raw/population/rvx/zonal_register_data/sdat3_d2023x_g2020.dbf',\n",
       "  'size_bytes': 2495779,\n",
       "  'size_mb': 2.38,\n",
       "  'extension': '.dbf'},\n",
       " {'filename': 'sdat3_d2023x_g2021.dbf',\n",
       "  'relative_path': 'sdat3_d2023x_g2021.dbf',\n",
       "  'full_path': '/Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/data/raw/population/rvx/zonal_register_data/sdat3_d2023x_g2021.dbf',\n",
       "  'size_bytes': 2495779,\n",
       "  'size_mb': 2.38,\n",
       "  'extension': '.dbf'},\n",
       " {'filename': 'sdat3_d2023x_g2023.dbf',\n",
       "  'relative_path': 'sdat3_d2023x_g2023.dbf',\n",
       "  'full_path': '/Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/data/raw/population/rvx/zonal_register_data/sdat3_d2023x_g2023.dbf',\n",
       "  'size_bytes': 2496487,\n",
       "  'size_mb': 2.38,\n",
       "  'extension': '.dbf'},\n",
       " {'filename': 'sdat3_d2023x_g2024.dbf',\n",
       "  'relative_path': 'sdat3_d2023x_g2024.dbf',\n",
       "  'full_path': '/Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/data/raw/population/rvx/zonal_register_data/sdat3_d2023x_g2024.dbf',\n",
       "  'size_bytes': 2496487,\n",
       "  'size_mb': 2.38,\n",
       "  'extension': '.dbf'},\n",
       " {'filename': 'sdat4_d2024_g2020.dbf',\n",
       "  'relative_path': 'sdat4_d2024_g2020.dbf',\n",
       "  'full_path': '/Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/data/raw/population/rvx/zonal_register_data/sdat4_d2024_g2020.dbf',\n",
       "  'size_bytes': 2721523,\n",
       "  'size_mb': 2.6,\n",
       "  'extension': '.dbf'},\n",
       " {'filename': 'sdat4_d2024_g2021.dbf',\n",
       "  'relative_path': 'sdat4_d2024_g2021.dbf',\n",
       "  'full_path': '/Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/data/raw/population/rvx/zonal_register_data/sdat4_d2024_g2021.dbf',\n",
       "  'size_bytes': 2721523,\n",
       "  'size_mb': 2.6,\n",
       "  'extension': '.dbf'},\n",
       " {'filename': 'sdat4_d2024_g2023.dbf',\n",
       "  'relative_path': 'sdat4_d2024_g2023.dbf',\n",
       "  'full_path': '/Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/data/raw/population/rvx/zonal_register_data/sdat4_d2024_g2023.dbf',\n",
       "  'size_bytes': 2722295,\n",
       "  'size_mb': 2.6,\n",
       "  'extension': '.dbf'},\n",
       " {'filename': 'sdat4_d2024_g2024.dbf',\n",
       "  'relative_path': 'sdat4_d2024_g2024.dbf',\n",
       "  'full_path': '/Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/data/raw/population/rvx/zonal_register_data/sdat4_d2024_g2024.dbf',\n",
       "  'size_bytes': 2722295,\n",
       "  'size_mb': 2.6,\n",
       "  'extension': '.dbf'},\n",
       " {'filename': 'sdat5_d2023_g2020.dbf',\n",
       "  'relative_path': 'sdat5_d2023_g2020.dbf',\n",
       "  'full_path': '/Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/data/raw/population/rvx/zonal_register_data/sdat5_d2023_g2020.dbf',\n",
       "  'size_bytes': 578171,\n",
       "  'size_mb': 0.55,\n",
       "  'extension': '.dbf'},\n",
       " {'filename': 'sdat5_d2023_g2021.dbf',\n",
       "  'relative_path': 'sdat5_d2023_g2021.dbf',\n",
       "  'full_path': '/Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/data/raw/population/rvx/zonal_register_data/sdat5_d2023_g2021.dbf',\n",
       "  'size_bytes': 578171,\n",
       "  'size_mb': 0.55,\n",
       "  'extension': '.dbf'},\n",
       " {'filename': 'sdat5_d2023_g2023.dbf',\n",
       "  'relative_path': 'sdat5_d2023_g2023.dbf',\n",
       "  'full_path': '/Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/data/raw/population/rvx/zonal_register_data/sdat5_d2023_g2023.dbf',\n",
       "  'size_bytes': 578335,\n",
       "  'size_mb': 0.55,\n",
       "  'extension': '.dbf'},\n",
       " {'filename': 'sdat5_d2023_g2024.dbf',\n",
       "  'relative_path': 'sdat5_d2023_g2024.dbf',\n",
       "  'full_path': '/Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/data/raw/population/rvx/zonal_register_data/sdat5_d2023_g2024.dbf',\n",
       "  'size_bytes': 578335,\n",
       "  'size_mb': 0.55,\n",
       "  'extension': '.dbf'},\n",
       " {'filename': 'sdat71_NB2023_grk2020_2020.dbf',\n",
       "  'relative_path': 'sdat71_NB2023_grk2020_2020.dbf',\n",
       "  'full_path': '/Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/data/raw/population/rvx/zonal_register_data/sdat71_NB2023_grk2020_2020.dbf',\n",
       "  'size_bytes': 408975,\n",
       "  'size_mb': 0.39,\n",
       "  'extension': '.dbf'},\n",
       " {'filename': 'sdat7_d20xx_g2020_ikke_pkost.dbf',\n",
       "  'relative_path': 'sdat7_d20xx_g2020_ikke_pkost.dbf',\n",
       "  'full_path': '/Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/data/raw/population/rvx/zonal_register_data/sdat7_d20xx_g2020_ikke_pkost.dbf',\n",
       "  'size_bytes': 690979,\n",
       "  'size_mb': 0.66,\n",
       "  'extension': '.dbf'},\n",
       " {'filename': 'sdat7_d20xx_g2021_ikke_pkost.dbf',\n",
       "  'relative_path': 'sdat7_d20xx_g2021_ikke_pkost.dbf',\n",
       "  'full_path': '/Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/data/raw/population/rvx/zonal_register_data/sdat7_d20xx_g2021_ikke_pkost.dbf',\n",
       "  'size_bytes': 690979,\n",
       "  'size_mb': 0.66,\n",
       "  'extension': '.dbf'},\n",
       " {'filename': 'sdat7_d20xx_g2023_ikke_pkost.dbf',\n",
       "  'relative_path': 'sdat7_d20xx_g2023_ikke_pkost.dbf',\n",
       "  'full_path': '/Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/data/raw/population/rvx/zonal_register_data/sdat7_d20xx_g2023_ikke_pkost.dbf',\n",
       "  'size_bytes': 691175,\n",
       "  'size_mb': 0.66,\n",
       "  'extension': '.dbf'},\n",
       " {'filename': 'sdat7_d20xx_g2024_ikke_pkost.dbf',\n",
       "  'relative_path': 'sdat7_d20xx_g2024_ikke_pkost.dbf',\n",
       "  'full_path': '/Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/data/raw/population/rvx/zonal_register_data/sdat7_d20xx_g2024_ikke_pkost.dbf',\n",
       "  'size_bytes': 691175,\n",
       "  'size_mb': 0.66,\n",
       "  'extension': '.dbf'},\n",
       " {'filename': 'sdat8_d2024_g2020.dbf',\n",
       "  'relative_path': 'sdat8_d2024_g2020.dbf',\n",
       "  'full_path': '/Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/data/raw/population/rvx/zonal_register_data/sdat8_d2024_g2020.dbf',\n",
       "  'size_bytes': 916595,\n",
       "  'size_mb': 0.87,\n",
       "  'extension': '.dbf'},\n",
       " {'filename': 'sdat8_d2024_g2021.dbf',\n",
       "  'relative_path': 'sdat8_d2024_g2021.dbf',\n",
       "  'full_path': '/Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/data/raw/population/rvx/zonal_register_data/sdat8_d2024_g2021.dbf',\n",
       "  'size_bytes': 916595,\n",
       "  'size_mb': 0.87,\n",
       "  'extension': '.dbf'},\n",
       " {'filename': 'sdat8_d2024_g2023.dbf',\n",
       "  'relative_path': 'sdat8_d2024_g2023.dbf',\n",
       "  'full_path': '/Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/data/raw/population/rvx/zonal_register_data/sdat8_d2024_g2023.dbf',\n",
       "  'size_bytes': 916855,\n",
       "  'size_mb': 0.87,\n",
       "  'extension': '.dbf'},\n",
       " {'filename': 'sdat8_d2024_g2024.dbf',\n",
       "  'relative_path': 'sdat8_d2024_g2024.dbf',\n",
       "  'full_path': '/Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/data/raw/population/rvx/zonal_register_data/sdat8_d2024_g2024.dbf',\n",
       "  'size_bytes': 916855,\n",
       "  'size_mb': 0.87,\n",
       "  'extension': '.dbf'},\n",
       " {'filename': 'sdat_6_areal_g2020.dbf',\n",
       "  'relative_path': 'sdat_6_areal_g2020.dbf',\n",
       "  'full_path': '/Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/data/raw/population/rvx/zonal_register_data/sdat_6_areal_g2020.dbf',\n",
       "  'size_bytes': 4173386,\n",
       "  'size_mb': 3.98,\n",
       "  'extension': '.dbf'}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbf_file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4130e3",
   "metadata": {},
   "source": [
    "## 7. Interactive Survey Data Inspector\n",
    "\n",
    "Browse through SAV (SPSS) files from the traveling_survey folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ec082adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìã AVAILABLE SAV FILES (Survey Data)\n",
      "================================================================================\n",
      "\n",
      "Change the sav_file_index variable below to inspect different files:\n",
      "\n",
      " 0. Nasjonal_RVU_PERSON_Nov26_0901.sav                 (63.65 MB, 100 rows)\n",
      " 1. Nasjonal_RVU_REISER_Nov26_0901.sav                 (64.36 MB, 100 rows)\n",
      " 2. RVU 2019-2024 Personfil Vektet 251125.sav          (93.56 MB, ? rows)\n",
      " 3. RVU 2019_2024 Reisefil 251107.sav                  (134.88 MB, 100 rows)\n",
      "\n",
      "Total: 4 SAV files\n",
      "\n",
      "Note: .docx files are documentation, .zip files are compressed archives\n"
     ]
    }
   ],
   "source": [
    "# List all available SAV files\n",
    "print(\"=\"*80)\n",
    "print(\"üìã AVAILABLE SAV FILES (Survey Data)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nChange the sav_file_index variable below to inspect different files:\\n\")\n",
    "\n",
    "sav_file_list = [f for f in all_files['traveling_survey'] if f['extension'].lower() == '.sav']\n",
    "\n",
    "for idx, f in enumerate(sav_file_list):\n",
    "    print(f\"{idx:2d}. {f['filename']:<50} ({f['size_mb']:.2f} MB, {file_schemas.get(f['relative_path'], {}).get('rows', '?')} rows)\")\n",
    "\n",
    "print(f\"\\nTotal: {len(sav_file_list)} SAV files\")\n",
    "print(\"\\nNote: .docx files are documentation, .zip files are compressed archives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d410802f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nasjonal_RVU_PERSON_Nov26_0901.sav (63.65 MB)\n",
      "\n",
      "51330 rows x 453 columns\n",
      "\n",
      "Schema check (columns + pandas type):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>443</th>\n",
       "      <th>444</th>\n",
       "      <th>445</th>\n",
       "      <th>446</th>\n",
       "      <th>447</th>\n",
       "      <th>448</th>\n",
       "      <th>449</th>\n",
       "      <th>450</th>\n",
       "      <th>451</th>\n",
       "      <th>452</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Column</th>\n",
       "      <td>altid</td>\n",
       "      <td>respid</td>\n",
       "      <td>altid_1</td>\n",
       "      <td>altidNumeric</td>\n",
       "      <td>PublishedVersion</td>\n",
       "      <td>date_time_start_1</td>\n",
       "      <td>date_time_start_2</td>\n",
       "      <td>postnummer</td>\n",
       "      <td>alder</td>\n",
       "      <td>Aldersgruppe</td>\n",
       "      <td>...</td>\n",
       "      <td>TRM_BRUK_PERSON_18</td>\n",
       "      <td>TRM_BRUK_PERSON_20</td>\n",
       "      <td>TRM_BRUK_PERSON_21</td>\n",
       "      <td>TRM_BRUK_PERSON_22</td>\n",
       "      <td>TRM_BRUK_PERSON_23</td>\n",
       "      <td>TRM_BRUK_PERSON_97</td>\n",
       "      <td>TRM_BRUK_PERSON_70</td>\n",
       "      <td>TRM_BRUK_PERSON_71</td>\n",
       "      <td>utvalgsvekt_kombinert_Q1_Q3</td>\n",
       "      <td>utvalgsvekt_nasjonal_Q1_Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>object</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>...</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows √ó 453 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0       1        2             3                 4    \\\n",
       "Column   altid  respid  altid_1  altidNumeric  PublishedVersion   \n",
       "Type    object  object   object       float64           float64   \n",
       "\n",
       "                      5                  6           7        8    \\\n",
       "Column  date_time_start_1  date_time_start_2  postnummer    alder   \n",
       "Type              float64            float64      object  float64   \n",
       "\n",
       "                 9    ...                 443                 444  \\\n",
       "Column  Aldersgruppe  ...  TRM_BRUK_PERSON_18  TRM_BRUK_PERSON_20   \n",
       "Type         float64  ...             float64             float64   \n",
       "\n",
       "                       445                 446                 447  \\\n",
       "Column  TRM_BRUK_PERSON_21  TRM_BRUK_PERSON_22  TRM_BRUK_PERSON_23   \n",
       "Type               float64             float64             float64   \n",
       "\n",
       "                       448                 449                 450  \\\n",
       "Column  TRM_BRUK_PERSON_97  TRM_BRUK_PERSON_70  TRM_BRUK_PERSON_71   \n",
       "Type               float64             float64             float64   \n",
       "\n",
       "                                451                         452  \n",
       "Column  utvalgsvekt_kombinert_Q1_Q3  utvalgsvekt_nasjonal_Q1_Q3  \n",
       "Type                        float64                     float64  \n",
       "\n",
       "[2 rows x 453 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>altid</th>\n",
       "      <th>respid</th>\n",
       "      <th>altid_1</th>\n",
       "      <th>altidNumeric</th>\n",
       "      <th>PublishedVersion</th>\n",
       "      <th>date_time_start_1</th>\n",
       "      <th>date_time_start_2</th>\n",
       "      <th>postnummer</th>\n",
       "      <th>alder</th>\n",
       "      <th>Aldersgruppe</th>\n",
       "      <th>...</th>\n",
       "      <th>TRM_BRUK_PERSON_18</th>\n",
       "      <th>TRM_BRUK_PERSON_20</th>\n",
       "      <th>TRM_BRUK_PERSON_21</th>\n",
       "      <th>TRM_BRUK_PERSON_22</th>\n",
       "      <th>TRM_BRUK_PERSON_23</th>\n",
       "      <th>TRM_BRUK_PERSON_97</th>\n",
       "      <th>TRM_BRUK_PERSON_70</th>\n",
       "      <th>TRM_BRUK_PERSON_71</th>\n",
       "      <th>utvalgsvekt_kombinert_Q1_Q3</th>\n",
       "      <th>utvalgsvekt_nasjonal_Q1_Q3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2225kj96</td>\n",
       "      <td>529507ea-efa4-4a0a-b39a-381f6ff2d9ae</td>\n",
       "      <td>2225kj96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20250505.0</td>\n",
       "      <td>73201.0</td>\n",
       "      <td>1396</td>\n",
       "      <td>37.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.052456</td>\n",
       "      <td>1.052456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>222ce9b3</td>\n",
       "      <td>3a5f1c25-0275-4804-81b2-1fce3b7514c3</td>\n",
       "      <td>222ce9b3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>20250521.0</td>\n",
       "      <td>161939.0</td>\n",
       "      <td>5310</td>\n",
       "      <td>40.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.819318</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>222mygj8</td>\n",
       "      <td>bc00ba46-9824-4328-810c-b18376aeb06b</td>\n",
       "      <td>222mygj8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20250411.0</td>\n",
       "      <td>145352.0</td>\n",
       "      <td>7072</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.845151</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22444jw3</td>\n",
       "      <td>ed53a78e-29dd-4160-a2c7-748d8abc1d23</td>\n",
       "      <td>22444jw3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>20250606.0</td>\n",
       "      <td>94329.0</td>\n",
       "      <td>9007</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.975325</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22476dch</td>\n",
       "      <td>914f2d07-2ed1-4c84-806f-8b0d20163108</td>\n",
       "      <td>22476dch</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>20250907.0</td>\n",
       "      <td>93902.0</td>\n",
       "      <td>4640</td>\n",
       "      <td>58.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.433040</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>224kguwm</td>\n",
       "      <td>bea0d537-7181-4a73-b5bc-d3461205f755</td>\n",
       "      <td>224kguwm</td>\n",
       "      <td>0.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>20250701.0</td>\n",
       "      <td>230049.0</td>\n",
       "      <td>9108</td>\n",
       "      <td>40.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.923852</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows √ó 453 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      altid                                respid   altid_1  altidNumeric  \\\n",
       "0  2225kj96  529507ea-efa4-4a0a-b39a-381f6ff2d9ae  2225kj96           0.0   \n",
       "1  222ce9b3  3a5f1c25-0275-4804-81b2-1fce3b7514c3  222ce9b3           0.0   \n",
       "2  222mygj8  bc00ba46-9824-4328-810c-b18376aeb06b  222mygj8           0.0   \n",
       "3  22444jw3  ed53a78e-29dd-4160-a2c7-748d8abc1d23  22444jw3           0.0   \n",
       "4  22476dch  914f2d07-2ed1-4c84-806f-8b0d20163108  22476dch           0.0   \n",
       "5  224kguwm  bea0d537-7181-4a73-b5bc-d3461205f755  224kguwm           0.0   \n",
       "\n",
       "   PublishedVersion  date_time_start_1  date_time_start_2 postnummer  alder  \\\n",
       "0              19.0         20250505.0            73201.0       1396   37.0   \n",
       "1             101.0         20250521.0           161939.0       5310   40.0   \n",
       "2              18.0         20250411.0           145352.0       7072   17.0   \n",
       "3             103.0         20250606.0            94329.0       9007   15.0   \n",
       "4             106.0         20250907.0            93902.0       4640   58.0   \n",
       "5             104.0         20250701.0           230049.0       9108   40.0   \n",
       "\n",
       "   Aldersgruppe  ...  TRM_BRUK_PERSON_18 TRM_BRUK_PERSON_20  \\\n",
       "0           3.0  ...                 0.0                0.0   \n",
       "1           4.0  ...                 0.0                0.0   \n",
       "2           1.0  ...                 0.0                0.0   \n",
       "3           1.0  ...                 0.0                0.0   \n",
       "4           5.0  ...                 0.0                0.0   \n",
       "5           4.0  ...                 0.0                0.0   \n",
       "\n",
       "   TRM_BRUK_PERSON_21  TRM_BRUK_PERSON_22  TRM_BRUK_PERSON_23  \\\n",
       "0                 0.0                 0.0                 0.0   \n",
       "1                 0.0                 0.0                 0.0   \n",
       "2                 0.0                 0.0                 0.0   \n",
       "3                 0.0                 0.0                 0.0   \n",
       "4                 0.0                 0.0                 0.0   \n",
       "5                 0.0                 0.0                 0.0   \n",
       "\n",
       "   TRM_BRUK_PERSON_97  TRM_BRUK_PERSON_70 TRM_BRUK_PERSON_71  \\\n",
       "0                 0.0                 0.0                0.0   \n",
       "1                 0.0                 0.0                0.0   \n",
       "2                 0.0                 1.0                1.0   \n",
       "3                 0.0                 0.0                0.0   \n",
       "4                 0.0                 0.0                0.0   \n",
       "5                 0.0                 0.0                0.0   \n",
       "\n",
       "  utvalgsvekt_kombinert_Q1_Q3 utvalgsvekt_nasjonal_Q1_Q3  \n",
       "0                    1.052456                   1.052456  \n",
       "1                    0.819318                        NaN  \n",
       "2                    0.845151                        NaN  \n",
       "3                    0.975325                        NaN  \n",
       "4                    1.433040                        NaN  \n",
       "5                    0.923852                        NaN  \n",
       "\n",
       "[6 rows x 453 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change this number to inspect different files (0 to 3)\n",
    "sav_file_index = 0\n",
    "\n",
    "# Load and display\n",
    "selected = sav_file_list[sav_file_index]\n",
    "df, meta = pyreadstat.read_sav(selected['full_path'])\n",
    "\n",
    "# Get actual total rows from metadata\n",
    "total_rows = meta.number_rows if hasattr(meta, 'number_rows') else len(df)\n",
    "\n",
    "print(f\"{selected['filename']} ({selected['size_mb']:.2f} MB)\\n\")\n",
    "print(f\"{total_rows} rows x {df.shape[1]} columns\\n\")\n",
    "\n",
    "# Schema check: column names and types\n",
    "col_names = list(df.columns)\n",
    "col_types = [str(df[col].dtype) for col in df.columns]\n",
    "\n",
    "schema_check = pd.DataFrame([col_names, col_types], index=[\"Column\", \"Type\"])\n",
    "print(\"Schema check (columns + pandas type):\")\n",
    "display(schema_check)\n",
    "\n",
    "# Raw preview\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4f79917e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26 SDAT files across 7 groups.\n",
      "\n",
      "================================================================================\n",
      "Group: sdat1 (4 files)\n",
      "Baseline: sdat1_d2024_g2020.dbf (41 columns)\n",
      "‚úÖ All files in this group have identical columns.\n",
      "\n",
      "================================================================================\n",
      "Group: sdat3 (4 files)\n",
      "Baseline: sdat3_d2023x_g2020.dbf (18 columns)\n",
      "‚úÖ All files in this group have identical columns.\n",
      "\n",
      "================================================================================\n",
      "Group: sdat4 (4 files)\n",
      "Baseline: sdat4_d2024_g2020.dbf (24 columns)\n",
      "‚úÖ All files in this group have identical columns.\n",
      "\n",
      "================================================================================\n",
      "Group: sdat5 (4 files)\n",
      "Baseline: sdat5_d2023_g2020.dbf (5 columns)\n",
      "‚úÖ All files in this group have identical columns.\n",
      "\n",
      "================================================================================\n",
      "Group: sdat7 (4 files)\n",
      "Baseline: sdat7_d20xx_g2020_ikke_pkost.dbf (6 columns)\n",
      "‚úÖ All files in this group have identical columns.\n",
      "\n",
      "================================================================================\n",
      "Group: sdat71 (1 files)\n",
      "Baseline: sdat71_NB2023_grk2020_2020.dbf (4 columns)\n",
      "‚úÖ All files in this group have identical columns.\n",
      "\n",
      "================================================================================\n",
      "Group: sdat8 (4 files)\n",
      "Baseline: sdat8_d2024_g2020.dbf (8 columns)\n",
      "‚úÖ All files in this group have identical columns.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Check if files with same SDAT number share the same columns\n",
    "sdat_files = [\n",
    "    f for f in all_files['zonal_register_data']\n",
    "    if f['filename'].lower().startswith('sdat') and f['extension'] == '.dbf'\n",
    "]\n",
    "\n",
    "# Group by exact prefix like sdat7_ vs sdat71_ (underscore boundary)\n",
    "groups = {}\n",
    "for f in sdat_files:\n",
    "    name = f['filename'].lower()\n",
    "    match = re.match(r\"^(sdat\\d+)_\", name)\n",
    "    if not match:\n",
    "        continue\n",
    "    key = match.group(1)  # e.g. sdat7, sdat71\n",
    "    groups.setdefault(key, []).append(f)\n",
    "\n",
    "print(f\"Found {len(sdat_files)} SDAT files across {len(groups)} groups.\")\n",
    "\n",
    "for group_key in sorted(groups.keys()):\n",
    "    group_files = groups[group_key]\n",
    "    schemas = {}\n",
    "    for f in group_files:\n",
    "        schema = file_schemas.get(f['relative_path'], {})\n",
    "        cols = [c['name'] for c in schema.get('columns_detail', [])]\n",
    "        schemas[f['filename']] = cols\n",
    "\n",
    "    base_file = group_files[0]['filename']\n",
    "    base_cols = schemas.get(base_file, [])\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Group: {group_key} ({len(group_files)} files)\")\n",
    "    print(f\"Baseline: {base_file} ({len(base_cols)} columns)\")\n",
    "\n",
    "    all_same = True\n",
    "    for fname, cols in schemas.items():\n",
    "        if cols != base_cols:\n",
    "            all_same = False\n",
    "            missing = [c for c in base_cols if c not in cols]\n",
    "            extra = [c for c in cols if c not in base_cols]\n",
    "            print(f\"\\nDifferences in {fname}:\")\n",
    "            if missing:\n",
    "                print(f\"  Missing: {missing}\")\n",
    "            if extra:\n",
    "                print(f\"  Extra: {extra}\")\n",
    "\n",
    "    if all_same:\n",
    "        print(\"‚úÖ All files in this group have identical columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1779f1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìä XLSX FILE COLUMN COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Total XLSX files: 2\n",
      "\n",
      "Found 1 groups\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Group: sdat2_data2020 (2 files)\n",
      "Baseline: sdat2_data2020_delomr.xlsx\n",
      "  Columns: 361\n",
      "‚ö†Ô∏è  sdat2_data2020_grunnkrets.xlsx: Different columns (361)\n",
      "     Missing: ['Delomr√•de']\n",
      "     Extra: ['grunnkrets']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Check XLSX files - group similar files and compare columns\n",
    "xlsx_files = [\n",
    "    f for f in all_files['zonal_register_data']\n",
    "    if f['extension'].lower() == '.xlsx'\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä XLSX FILE COLUMN COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal XLSX files: {len(xlsx_files)}\")\n",
    "\n",
    "# Group XLSX files by prefix (e.g., sdat2_data2020)\n",
    "groups = {}\n",
    "for f in xlsx_files:\n",
    "    name = f['filename'].lower()\n",
    "    # Extract prefix like \"sdat2_data2020\"\n",
    "    match = re.match(r\"^(sdat\\d+_data\\d+)_\", name)\n",
    "    if match:\n",
    "        key = match.group(1)\n",
    "    else:\n",
    "        key = name.split('_')[0] if '_' in name else name.split('.')[0]\n",
    "    groups.setdefault(key, []).append(f)\n",
    "\n",
    "print(f\"\\nFound {len(groups)} groups\\n\")\n",
    "\n",
    "for group_key in sorted(groups.keys()):\n",
    "    group_files = groups[group_key]\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Group: {group_key} ({len(group_files)} files)\")\n",
    "    \n",
    "    # Get columns for each file\n",
    "    schemas = {}\n",
    "    for f in group_files:\n",
    "        key = f['relative_path']\n",
    "        schema = file_schemas.get(key, {})\n",
    "        sheets = schema.get('sheets', [])\n",
    "        if sheets:\n",
    "            # Get columns from first sheet\n",
    "            col_names = sheets[0].get('column_names', [])\n",
    "            schemas[f['filename']] = col_names\n",
    "    \n",
    "    # Compare columns\n",
    "    if len(group_files) > 1:\n",
    "        base_file = group_files[0]['filename']\n",
    "        base_cols = schemas.get(base_file, [])\n",
    "        \n",
    "        print(f\"Baseline: {base_file}\")\n",
    "        print(f\"  Columns: {len(base_cols)}\")\n",
    "        \n",
    "        all_same = True\n",
    "        for f in group_files[1:]:\n",
    "            cols = schemas.get(f['filename'], [])\n",
    "            if cols == base_cols:\n",
    "                print(f\"‚úÖ {f['filename']}: Identical columns ({len(cols)})\")\n",
    "            else:\n",
    "                all_same = False\n",
    "                print(f\"‚ö†Ô∏è  {f['filename']}: Different columns ({len(cols)})\")\n",
    "                missing = [c for c in base_cols if c not in cols]\n",
    "                extra = [c for c in cols if c not in base_cols]\n",
    "                if missing:\n",
    "                    print(f\"     Missing: {missing}\")\n",
    "                if extra:\n",
    "                    print(f\"     Extra: {extra}\")\n",
    "        \n",
    "        if all_same:\n",
    "            print(f\"\\n‚úÖ All {len(group_files)} files in this group have identical columns!\")\n",
    "    else:\n",
    "        f = group_files[0]\n",
    "        cols = schemas.get(f['filename'], [])\n",
    "        print(f\"{f['filename']}: {len(cols)} columns\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_synthetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
