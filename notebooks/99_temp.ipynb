{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38d5b7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COLUMN CHARACTERISTICS ===\n",
      "\n",
      "orgn2025:\n",
      "  Total records: 5000\n",
      "  Unique values: 1059\n",
      "  Uniqueness: 21.2%\n",
      "  Examples: ['', '', '999557244']\n",
      "\n",
      "navn2025:\n",
      "  Total records: 5000\n",
      "  Unique values: 1061\n",
      "  Uniqueness: 21.2%\n",
      "  Examples: ['', '', 'SOUNDS LIKE NORWAY']\n",
      "\n",
      "fpst2025:\n",
      "  Total records: 5000\n",
      "  Unique values: 444\n",
      "  Uniqueness: 8.9%\n",
      "  Examples: ['', '', 'KONGSBERG']\n",
      "\n",
      "fpnr2025:\n",
      "  Total records: 5000\n",
      "  Unique values: 775\n",
      "  Uniqueness: 15.5%\n",
      "  Examples: ['', '', '3616']\n",
      "\n",
      "SN2025:\n",
      "  Total records: 5000\n",
      "  Unique values: 268\n",
      "  Uniqueness: 5.4%\n",
      "  Examples: ['', '', '90.020']\n",
      "\n",
      "orgf2025:\n",
      "  Total records: 5000\n",
      "  Unique values: 18\n",
      "  Uniqueness: 0.4%\n",
      "  Examples: ['', '', 'NUF']\n",
      "\n",
      "=== POSTAL CODE vs POSTAL NAME ===\n",
      "Unique postal codes (fpnr2025): 775\n",
      "Unique postal names (fpst2025): 444\n",
      "Mapping combinations: 796\n",
      "\n",
      "Sample mappings:\n",
      "fpnr2025  fpst2025\n",
      "                  \n",
      "    3616 KONGSBERG\n",
      "    2052  JESSHEIM\n",
      "    4770     HÃ˜VÃ…G\n",
      "    2165      HVAM\n",
      "    5118     ULSET\n",
      "    0152      OSLO\n",
      "    3921 PORSGRUNN\n",
      "    2500    TYNSET\n",
      "    0978      OSLO\n"
     ]
    }
   ],
   "source": [
    "import pyreadstat\n",
    "import pandas as pd\n",
    "\n",
    "# Load sample of data\n",
    "df, meta = pyreadstat.read_sav(\n",
    "    '../data/raw/population/geostad/VoF2025_geokodet.SAV',\n",
    "    usecols=['orgn2025', 'navn2025', 'fpst2025', 'fpnr2025', 'SN2025', 'orgf2025'],\n",
    "    row_limit=5000\n",
    ")\n",
    "\n",
    "print(\"=== COLUMN CHARACTERISTICS ===\\n\")\n",
    "\n",
    "# Analyze each column\n",
    "for col in ['orgn2025', 'navn2025', 'fpst2025', 'fpnr2025', 'SN2025', 'orgf2025']:\n",
    "    print(f\"{col}:\")\n",
    "    print(f\"  Total records: {len(df)}\")\n",
    "    print(f\"  Unique values: {df[col].nunique()}\")\n",
    "    print(f\"  Uniqueness: {df[col].nunique() / len(df) * 100:.1f}%\")\n",
    "    print(f\"  Examples: {df[col].dropna().head(3).tolist()}\")\n",
    "    print()\n",
    "\n",
    "# Check relationship between fpnr2025 and fpst2025\n",
    "print(\"=== POSTAL CODE vs POSTAL NAME ===\")\n",
    "postal_mapping = df[['fpnr2025', 'fpst2025']].drop_duplicates()\n",
    "print(f\"Unique postal codes (fpnr2025): {df['fpnr2025'].nunique()}\")\n",
    "print(f\"Unique postal names (fpst2025): {df['fpst2025'].nunique()}\")\n",
    "print(f\"Mapping combinations: {len(postal_mapping)}\")\n",
    "print(\"\\nSample mappings:\")\n",
    "print(postal_mapping.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3adc643a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': 'sdat3_d2023x_g2020.dbf',\n",
       " 'num_records': 14097,\n",
       " 'encoding': 'ascii',\n",
       " 'fields': [{'name': 'GRUNNKRETS',\n",
       "   'type': 'N',\n",
       "   'length': 8,\n",
       "   'decimal_count': 0},\n",
       "  {'name': 'SYBLU', 'type': 'N', 'length': 10, 'decimal_count': 4},\n",
       "  {'name': 'SYBMU', 'type': 'N', 'length': 10, 'decimal_count': 4},\n",
       "  {'name': 'SYBHU', 'type': 'N', 'length': 10, 'decimal_count': 4},\n",
       "  {'name': 'SYB', 'type': 'N', 'length': 10, 'decimal_count': 4},\n",
       "  {'name': 'SYALU', 'type': 'N', 'length': 10, 'decimal_count': 4},\n",
       "  {'name': 'SYAMU', 'type': 'N', 'length': 10, 'decimal_count': 4},\n",
       "  {'name': 'SYAHU', 'type': 'N', 'length': 10, 'decimal_count': 4},\n",
       "  {'name': 'SYA1524', 'type': 'N', 'length': 10, 'decimal_count': 4},\n",
       "  {'name': 'SYA2534', 'type': 'N', 'length': 10, 'decimal_count': 4},\n",
       "  {'name': 'SYA3554', 'type': 'N', 'length': 10, 'decimal_count': 4},\n",
       "  {'name': 'SYA5566', 'type': 'N', 'length': 10, 'decimal_count': 4},\n",
       "  {'name': 'SYA67UP', 'type': 'N', 'length': 10, 'decimal_count': 4},\n",
       "  {'name': 'SYAM', 'type': 'N', 'length': 10, 'decimal_count': 4},\n",
       "  {'name': 'SYAK', 'type': 'N', 'length': 10, 'decimal_count': 4},\n",
       "  {'name': 'SYA', 'type': 'N', 'length': 10, 'decimal_count': 4},\n",
       "  {'name': 'INNT_IDX', 'type': 'N', 'length': 10, 'decimal_count': 4},\n",
       "  {'name': 'BRINNT17UP', 'type': 'N', 'length': 8, 'decimal_count': 0}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dbfread import DBF\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to the raw DBF file\n",
    "data_path = Path(\n",
    "    \"/Users/anderskielland/Documents/Synthetic data/code/synthetic-lab/\"\n",
    "    \"data/raw/population/tramod/sdat3_d2023x_g2020.dbf\"\n",
    ")\n",
    "\n",
    "# Load DBF (no records loaded yet)\n",
    "table = DBF(data_path, load=False)\n",
    "\n",
    "# --- Metadata ---\n",
    "metadata = {\n",
    "    \"file_name\": data_path.name,\n",
    "    \"num_records\": table.header.numrecords,\n",
    "    \"encoding\": table.encoding,\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": field.name,\n",
    "            \"type\": field.type,\n",
    "            \"length\": field.length,\n",
    "            \"decimal_count\": field.decimal_count,\n",
    "        }\n",
    "        for field in table.fields\n",
    "    ],\n",
    "}\n",
    "\n",
    "metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17e87d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INNT_IDX Field Analysis ===\n",
      "\n",
      "Data type: float64\n",
      "Total records: 14097\n",
      "Unique values: 72\n",
      "Missing values: 0\n",
      "\n",
      "Value counts:\n",
      "INNT_IDX\n",
      "0.91    773\n",
      "0.96    740\n",
      "0.97    734\n",
      "0.94    660\n",
      "0.93    658\n",
      "0.95    625\n",
      "0.92    583\n",
      "0.90    567\n",
      "0.98    545\n",
      "0.99    540\n",
      "0.86    500\n",
      "0.88    474\n",
      "1.03    468\n",
      "0.89    454\n",
      "0.87    439\n",
      "1.00    413\n",
      "1.01    375\n",
      "1.02    323\n",
      "1.04    309\n",
      "0.85    284\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Basic statistics:\n",
      "count    14097.000000\n",
      "mean         0.970969\n",
      "std          0.106760\n",
      "min          0.000000\n",
      "25%          0.900000\n",
      "50%          0.960000\n",
      "75%          1.020000\n",
      "max          1.500000\n",
      "Name: INNT_IDX, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the DBF file into a pandas DataFrame\n",
    "df = pd.DataFrame(iter(DBF(data_path)))\n",
    "\n",
    "# Analyze INNT_IDX field\n",
    "print(\"=== INNT_IDX Field Analysis ===\\n\")\n",
    "print(f\"Data type: {df['INNT_IDX'].dtype}\")\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(f\"Unique values: {df['INNT_IDX'].nunique()}\")\n",
    "print(f\"Missing values: {df['INNT_IDX'].isna().sum()}\\n\")\n",
    "\n",
    "print(\"Value counts:\")\n",
    "print(df['INNT_IDX'].value_counts().head(20))\n",
    "\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df['INNT_IDX'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22ca76a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Primary Statistics for INNT_IDX ===\n",
      "\n",
      "Count                      14,097.00\n",
      "Min                             0.00\n",
      "Max                             1.50\n",
      "Mean                            0.97\n",
      "Median                          0.96\n",
      "Std Dev                         0.11\n",
      "25th Percentile                 0.90\n",
      "75th Percentile                 1.02\n"
     ]
    }
   ],
   "source": [
    "# Primary statistics for INNT_IDX\n",
    "innt_idx_stats = {\n",
    "    \"Count\": df['INNT_IDX'].count(),\n",
    "    \"Min\": df['INNT_IDX'].min(),\n",
    "    \"Max\": df['INNT_IDX'].max(),\n",
    "    \"Mean\": df['INNT_IDX'].mean(),\n",
    "    \"Median\": df['INNT_IDX'].median(),\n",
    "    \"Std Dev\": df['INNT_IDX'].std(),\n",
    "    \"25th Percentile\": df['INNT_IDX'].quantile(0.25),\n",
    "    \"75th Percentile\": df['INNT_IDX'].quantile(0.75),\n",
    "}\n",
    "\n",
    "print(\"=== Primary Statistics for INNT_IDX ===\\n\")\n",
    "for key, value in innt_idx_stats.items():\n",
    "    print(f\"{key:20} {value:>15,.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67565c16",
   "metadata": {},
   "source": [
    "## Post-Synthesis Reconstruction Example\n",
    "\n",
    "This shows how to reconstruct postal names after generating synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "987987b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Modules reloaded\n"
     ]
    }
   ],
   "source": [
    "# Restart kernel and reimport to get latest code\n",
    "import importlib\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Clear any cached modules\n",
    "if 'synlab.data.population_geostad' in sys.modules:\n",
    "    del sys.modules['synlab.data.population_geostad']\n",
    "if 'synlab.data.configs' in sys.modules:\n",
    "    del sys.modules['synlab.data.configs']\n",
    "\n",
    "from synlab.data.population_geostad import (\n",
    "    load_geostad, \n",
    "    create_postal_mapping, \n",
    "    reconstruct_postal_names\n",
    ")\n",
    "from synlab.data.configs import GeoSTADConfig\n",
    "\n",
    "print(\"âœ“ Modules reloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2dcd4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to geocoded businesses: 770,568 / 2,252,561 rows\n",
      "Removed 541 duplicate rows: 770,027 rows remaining\n",
      "Loaded real data columns: ['SN2025', 'orgf2025', 'fpnr2025', 'grk2025', 'X_2025', 'Y_2025', 'orgn2025', 'navn2025']\n",
      "Created postal mapping with 4470 unique postal codes\n",
      "\n",
      "Example mappings: [('', \"5222 AT's Hertoge\"), ('3616', ''), ('2052', ''), ('4770', ''), ('2165', '')]\n",
      "\n",
      "Synthetic data columns: ['SN2025', 'orgf2025', 'fpnr2025', 'grk2025', 'X_2025', 'Y_2025']\n",
      "Notice: fpst2025 is NOT in synthetic data (only fpnr2025)\n",
      "Successfully reconstructed postal names for all 100 synthetic records\n",
      "\n",
      "After reconstruction: ['SN2025', 'orgf2025', 'fpnr2025', 'grk2025', 'X_2025', 'Y_2025', 'fpst2025']\n",
      "\n",
      "Sample of reconstructed data:\n",
      "        fpnr2025     fpst2025\n",
      "1033118     6856       BERGEN\n",
      "1031064     0196             \n",
      "2221086     0755         OSLO\n",
      "201050      3153             \n",
      "1362883     4050         SAND\n",
      "657648      2816         KAPP\n",
      "491729      9303             \n",
      "930121      2619             \n",
      "962913      3414  LIERSTRANDA\n",
      "404207      5136             \n"
     ]
    }
   ],
   "source": [
    "# 1. Load real data (only features, no postal names)\n",
    "config = GeoSTADConfig()\n",
    "df_real, domain = load_geostad(config)\n",
    "print(f\"Loaded real data columns: {df_real.columns.tolist()}\")\n",
    "\n",
    "# 2. Create postal code â†’ name mapping from raw data file\n",
    "postal_mapping = create_postal_mapping(config)\n",
    "print(f\"\\nExample mappings: {list(postal_mapping.items())[:5]}\")\n",
    "\n",
    "# 3. Simulate synthetic data (for demo, we'll just use a sample)\n",
    "# In practice, this would be output from DPMM-MST or another synthesizer\n",
    "df_synthetic = df_real[config.feature_columns].sample(100, random_state=42)\n",
    "print(f\"\\nSynthetic data columns: {df_synthetic.columns.tolist()}\")\n",
    "print(f\"Notice: fpst2025 is NOT in synthetic data (only fpnr2025)\")\n",
    "\n",
    "# 4. Reconstruct postal names in synthetic data\n",
    "df_synthetic_complete = reconstruct_postal_names(df_synthetic, postal_mapping)\n",
    "print(f\"\\nAfter reconstruction: {df_synthetic_complete.columns.tolist()}\")\n",
    "print(f\"\\nSample of reconstructed data:\")\n",
    "print(df_synthetic_complete[['fpnr2025', 'fpst2025']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024c261a",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "1. **During synthesis**: Exclude `fpst2025` (postal name) to avoid redundancy with `fpnr2025` (postal code)\n",
    "2. **After synthesis**: Reconstruct `fpst2025` using `create_postal_mapping()` and `reconstruct_postal_names()`\n",
    "3. **Why this works**: Postal names are deterministically derived from postal codes, so we can map them back after synthesis\n",
    "4. **Result**: Synthetic data is more comparable to original dataset (has same columns) while maintaining proper statistical properties\n",
    "\n",
    "Note: Some empty postal names in the mapping are from the original data - these will be preserved in reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81916c1",
   "metadata": {},
   "source": [
    "## Config-Driven Reconstruction (NEW Approach)\n",
    "\n",
    "The config now specifies which columns are derived and should be reconstructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4deb986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config specifies these derived columns:\n",
      "  fpst2025 â† fpnr2025\n",
      "\n",
      "============================================================\n",
      "Created mapping for fpst2025 from fpnr2025: 4470 unique values\n",
      "\n",
      "============================================================\n",
      "Filtered to geocoded businesses: 770,568 / 2,252,561 rows\n",
      "Removed 541 duplicate rows: 770,027 rows remaining\n",
      "\n",
      "Synthetic data columns: ['SN2025', 'orgf2025', 'fpnr2025', 'grk2025', 'X_2025', 'Y_2025']\n",
      "\n",
      "============================================================\n",
      "âœ“ Reconstructed fpst2025 from fpnr2025 for all 100 records\n",
      "\n",
      "Final columns: ['SN2025', 'orgf2025', 'fpnr2025', 'grk2025', 'X_2025', 'Y_2025', 'fpst2025']\n",
      "\n",
      "Sample:\n",
      "        fpnr2025 fpst2025\n",
      "1033118     6856   BERGEN\n",
      "1031064     0196         \n",
      "2221086     0755     OSLO\n",
      "201050      3153         \n",
      "1362883     4050     SAND\n",
      "657648      2816     KAPP\n",
      "491729      9303         \n",
      "930121      2619         \n"
     ]
    }
   ],
   "source": [
    "# Reload to get updated functions\n",
    "if 'synlab.data.population_geostad' in sys.modules:\n",
    "    del sys.modules['synlab.data.population_geostad']\n",
    "if 'synlab.data.configs' in sys.modules:\n",
    "    del sys.modules['synlab.data.configs']\n",
    "\n",
    "from synlab.data.population_geostad import (\n",
    "    load_geostad, \n",
    "    create_derived_mappings, \n",
    "    reconstruct_derived_columns\n",
    ")\n",
    "from synlab.data.configs import GeoSTADConfig\n",
    "\n",
    "# Check config's derived_columns specification\n",
    "config = GeoSTADConfig()\n",
    "print(f\"Config specifies these derived columns:\")\n",
    "for derived, source in config.derived_columns.items():\n",
    "    print(f\"  {derived} â† {source}\")\n",
    "\n",
    "# Create mappings based on config\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "mappings = create_derived_mappings(config)\n",
    "\n",
    "# Load and synthesize (simulated)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "df_real, domain = load_geostad(config)\n",
    "df_synthetic = df_real[config.feature_columns].sample(100, random_state=42)\n",
    "print(f\"\\nSynthetic data columns: {df_synthetic.columns.tolist()}\")\n",
    "\n",
    "# Reconstruct using config\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "df_complete = reconstruct_derived_columns(df_synthetic, mappings, config)\n",
    "print(f\"\\nFinal columns: {df_complete.columns.tolist()}\")\n",
    "print(f\"\\nSample:\")\n",
    "print(df_complete[['fpnr2025', 'fpst2025']].head(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3252baf",
   "metadata": {},
   "source": [
    "## Updated Import Location\n",
    "\n",
    "The generic reconstruction functions are now in `synlab.data.reconstruction` module, not GeoSTAD-specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2fde70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Imports from generic reconstruction module\n",
      "Created mapping for fpst2025 from fpnr2025: 4470 unique values\n",
      "\n",
      "Mappings created: ['fpst2025']\n",
      "\n",
      "These are generic functions usable with RVU, TraMOD, or any dataset config.\n"
     ]
    }
   ],
   "source": [
    "# Clean imports - reconstruction functions are now generic\n",
    "if 'synlab.data.reconstruction' in sys.modules:\n",
    "    del sys.modules['synlab.data.reconstruction']\n",
    "if 'synlab.data.population_geostad' in sys.modules:\n",
    "    del sys.modules['synlab.data.population_geostad']\n",
    "if 'synlab.data.configs' in sys.modules:\n",
    "    del sys.modules['synlab.data.configs']\n",
    "\n",
    "# Import from the correct module - generic functions in reconstruction\n",
    "from synlab.data.reconstruction import (\n",
    "    create_derived_mappings,\n",
    "    reconstruct_derived_columns\n",
    ")\n",
    "from synlab.data.population_geostad import load_geostad\n",
    "from synlab.data.configs import GeoSTADConfig\n",
    "\n",
    "# Or use the convenience imports from synlab.data\n",
    "# from synlab.data import (\n",
    "#     create_derived_mappings,\n",
    "#     reconstruct_derived_columns,\n",
    "#     load_geostad,\n",
    "#     GeoSTADConfig\n",
    "# )\n",
    "\n",
    "print(\"âœ“ Imports from generic reconstruction module\")\n",
    "\n",
    "# Test with GeoSTAD config\n",
    "config = GeoSTADConfig()\n",
    "mappings = create_derived_mappings(config)\n",
    "print(f\"\\nMappings created: {list(mappings.keys())}\")\n",
    "\n",
    "# These functions work with ANY config that has derived_columns, not just GeoSTAD!\n",
    "print(f\"\\nThese are generic functions usable with RVU, TraMOD, or any dataset config.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c2c2db",
   "metadata": {},
   "source": [
    "## Module Organization Discussion\n",
    "\n",
    "**Current folders:**\n",
    "- `synlab/core/` - Abstract base classes (BaseSynthesizer, ExperimentConfig)\n",
    "- `synlab/data/` - Data loaders, configs, preprocessing\n",
    "- `synlab/utils/` - Simple utilities (paths.py)\n",
    "- `synlab/methods/` - Synthesis algorithms (DPMM-MST)\n",
    "- `synlab/evaluation/` - Metrics\n",
    "- `synlab/runners/` - End-to-end pipelines\n",
    "\n",
    "**Where should reconstruction functions go?**\n",
    "\n",
    "**Option A: `synlab/utils/postprocessing.py` (RECOMMENDED)**\n",
    "- Generic post-synthesis utilities\n",
    "- Not tightly coupled to data loading\n",
    "- Parallel to other utils\n",
    "\n",
    "**Option B: Keep in `synlab/data/reconstruction.py`**\n",
    "- Grouped with configs\n",
    "- Related to data transformation\n",
    "- Current location\n",
    "\n",
    "**Option C: `synlab/core/postprocessing.py`**\n",
    "- Part of synthesis pipeline infrastructure\n",
    "- But not an abstract interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "612c5af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Successfully imported from synlab.utils.postprocessing\n",
      "Config has derived_columns: {'fpst2025': 'fpnr2025'}\n",
      "Created mapping for fpst2025 from fpnr2025: 4470 unique values\n",
      "âœ“ Created mappings: ['fpst2025']\n",
      "\n",
      "âœ“ Better module organization - postprocessing utilities in utils folder!\n"
     ]
    }
   ],
   "source": [
    "# Test new import location - clear ALL synlab modules\n",
    "modules_to_delete = [k for k in sys.modules.keys() if k.startswith('synlab')]\n",
    "for mod in modules_to_delete:\n",
    "    del sys.modules[mod]\n",
    "\n",
    "# New location: synlab.utils.postprocessing\n",
    "from synlab.utils.postprocessing import (\n",
    "    create_derived_mappings,\n",
    "    reconstruct_derived_columns\n",
    ")\n",
    "from synlab.data import load_geostad, GeoSTADConfig\n",
    "\n",
    "print(\"âœ“ Successfully imported from synlab.utils.postprocessing\")\n",
    "\n",
    "# Test it works\n",
    "config = GeoSTADConfig()\n",
    "print(f\"Config has derived_columns: {config.derived_columns}\")\n",
    "mappings = create_derived_mappings(config)\n",
    "print(f\"âœ“ Created mappings: {list(mappings.keys())}\")\n",
    "print(\"\\nâœ“ Better module organization - postprocessing utilities in utils folder!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02b6e41",
   "metadata": {},
   "source": [
    "## Data Module Organization Patterns\n",
    "\n",
    "### Current Structure: `population_geostad.py`\n",
    "\n",
    "The file currently contains:\n",
    "1. **Loading** - `load_geostad()` reads SPSS file\n",
    "2. **Cleaning** - removes duplicates, filters geocoded records\n",
    "3. **Metadata extraction** - `_build_domain_metadata()` \n",
    "4. **Preprocessing** - `prepare_for_synthesis()` selects feature columns\n",
    "\n",
    "### Is this common? Depends on project size:\n",
    "\n",
    "**For small/medium projects (like yours):** âœ… **TOTALLY FINE**\n",
    "- All operations are dataset-specific\n",
    "- Functions are clearly separated\n",
    "- Easy to navigate (~200 lines)\n",
    "- Common pattern for research/analysis projects\n",
    "\n",
    "**For large production systems:** Consider splitting into:\n",
    "```\n",
    "synlab/data/\n",
    "â”œâ”€â”€ loaders/\n",
    "â”‚   â”œâ”€â”€ geostad_loader.py      # Raw file reading\n",
    "â”‚   â””â”€â”€ rvu_loader.py\n",
    "â”œâ”€â”€ cleaners/\n",
    "â”‚   â”œâ”€â”€ geostad_cleaner.py     # Deduplication, filtering\n",
    "â”‚   â””â”€â”€ rvu_cleaner.py\n",
    "â”œâ”€â”€ transformers/\n",
    "â”‚   â”œâ”€â”€ geostad_transformer.py # Feature engineering\n",
    "â”‚   â””â”€â”€ common.py               # Shared preprocessing\n",
    "â””â”€â”€ configs.py\n",
    "```\n",
    "\n",
    "**When to split:**\n",
    "- File > 500 lines\n",
    "- Multiple datasets share cleaning logic\n",
    "- Need to test each step independently\n",
    "- Team size > 3-5 people"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a556cbaf",
   "metadata": {},
   "source": [
    "### Industry Best Practices\n",
    "\n",
    "**Simple is better than complex:**\n",
    "- pandas creator Wes McKinney often combines loading + basic cleaning in one module\n",
    "- scikit-learn's datasets module does loading + preprocessing together\n",
    "- Many research projects (statsmodels, scipy) use this pattern\n",
    "\n",
    "**Signs you SHOULD split:**\n",
    "\n",
    "1. **Repeated logic across datasets:**\n",
    "   ```python\n",
    "   # If you're doing this in multiple files:\n",
    "   df = df.drop_duplicates()\n",
    "   df = df.dropna(subset=key_cols)\n",
    "   ```\n",
    "   â†’ Move to `synlab/data/cleaners/common.py`\n",
    "\n",
    "2. **Complex transformations:**\n",
    "   ```python\n",
    "   # If preprocessing becomes >100 lines with feature engineering:\n",
    "   df['log_income'] = np.log1p(df['income'])\n",
    "   df['age_group'] = pd.cut(df['age'], bins=[0,18,30,50,65,100])\n",
    "   # ... many more transformations\n",
    "   ```\n",
    "   â†’ Create `synlab/data/transformers/`\n",
    "\n",
    "3. **Different cleaning strategies:**\n",
    "   ```python\n",
    "   # If you want to experiment with different cleaning approaches:\n",
    "   clean_aggressive()\n",
    "   clean_conservative()\n",
    "   clean_for_ml()\n",
    "   ```\n",
    "   â†’ Separate cleaner modules\n",
    "\n",
    "### Your Current Structure: ğŸ‘ Good for now!\n",
    "\n",
    "**Strengths:**\n",
    "- Clear function separation (`load_geostad()` vs `prepare_for_synthesis()`)\n",
    "- Config-driven (easy to modify behavior)\n",
    "- Well-documented\n",
    "- ~200 lines (manageable)\n",
    "\n",
    "**Consider splitting when:**\n",
    "- You add RVU loader and notice duplicate cleaning code\n",
    "- File exceeds 500 lines\n",
    "- You need multiple preprocessing pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e18e258e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Current Functions in population_geostad.py ===\n",
      "\n",
      "load_geostad(config: Optional[synlab.data.configs.GeoSTADConfig] = None) -> Tuple[pandas.core.frame.DataFrame, Dict[str, Any]]\n",
      "  Purpose: Load the GeoSTAD business registry dataset using configuration.\n",
      "\n",
      "prepare_for_synthesis(df: pandas.core.frame.DataFrame, config: Optional[synlab.data.configs.GeoSTADConfig] = None, sample_size: Optional[int] = None, random_state: int = 42) -> pandas.core.frame.DataFrame\n",
      "  Purpose: Prepare GeoSTAD data for synthetic generation.\n",
      "\n",
      "âœ“ Clean separation of concerns:\n",
      "  - load_geostad(): Loading + basic cleaning (dedup, filtering)\n",
      "  - prepare_for_synthesis(): Feature selection for synthesis\n",
      "\n",
      "âœ“ Both are GeoSTAD-specific, so keeping them together makes sense!\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate the current clean structure\n",
    "import inspect\n",
    "from synlab.data.population_geostad import load_geostad, prepare_for_synthesis\n",
    "\n",
    "print(\"=== Current Functions in population_geostad.py ===\\n\")\n",
    "\n",
    "# Show function signatures\n",
    "functions = [load_geostad, prepare_for_synthesis]\n",
    "for func in functions:\n",
    "    sig = inspect.signature(func)\n",
    "    doc = func.__doc__.split('\\n')[1].strip() if func.__doc__ else \"No description\"\n",
    "    print(f\"{func.__name__}{sig}\")\n",
    "    print(f\"  Purpose: {doc}\")\n",
    "    print()\n",
    "\n",
    "print(\"âœ“ Clean separation of concerns:\")\n",
    "print(\"  - load_geostad(): Loading + basic cleaning (dedup, filtering)\")\n",
    "print(\"  - prepare_for_synthesis(): Feature selection for synthesis\")\n",
    "print(\"\\nâœ“ Both are GeoSTAD-specific, so keeping them together makes sense!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d017e16a",
   "metadata": {},
   "source": [
    "### Real Example: When You'll Need to Split\n",
    "\n",
    "When you implement the RVU dataset, you might see:\n",
    "\n",
    "**BAD (duplicate code):**\n",
    "```python\n",
    "# population_geostad.py\n",
    "def load_geostad(config):\n",
    "    df = load_raw(config.raw_path)\n",
    "    df = df.drop_duplicates()  # â† duplicate\n",
    "    df = df.dropna(subset=key_cols)  # â† duplicate\n",
    "    return df\n",
    "\n",
    "# population_rvu.py  \n",
    "def load_rvu(config):\n",
    "    df = load_raw(config.raw_path)\n",
    "    df = df.drop_duplicates()  # â† same code!\n",
    "    df = df.dropna(subset=key_cols)  # â† same code!\n",
    "    return df\n",
    "```\n",
    "\n",
    "**GOOD (extract common cleaning):**\n",
    "```python\n",
    "# data/cleaners/common.py\n",
    "def clean_dataframe(df, key_cols):\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.dropna(subset=key_cols)\n",
    "    return df\n",
    "\n",
    "# population_geostad.py\n",
    "def load_geostad(config):\n",
    "    df = load_raw(config.raw_path)\n",
    "    df = clean_dataframe(df, ['X_2025', 'Y_2025'])\n",
    "    return df\n",
    "\n",
    "# population_rvu.py\n",
    "def load_rvu(config):\n",
    "    df = load_raw(config.raw_path)\n",
    "    df = clean_dataframe(df, ['grunnkrets'])\n",
    "    return df\n",
    "```\n",
    "\n",
    "**Recommendation:** Wait until you see the pattern repeat 2-3 times before abstracting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb17035",
   "metadata": {},
   "source": [
    "## Loading vs Preprocessing vs Transformation - Deep Dive\n",
    "\n",
    "### Definitions (with your GeoSTAD examples):\n",
    "\n",
    "**1. Loading (Raw â†’ Clean DataFrame)**\n",
    "- Reading file formats (SPSS, DBF, CSV)\n",
    "- Basic data quality: deduplication, null handling\n",
    "- Type conversion (strings to numbers)\n",
    "- Column selection\n",
    "- **Output:** Clean, validated DataFrame\n",
    "\n",
    "**2. Preprocessing (Clean â†’ Model-Ready)**\n",
    "- Feature engineering (derived columns)\n",
    "- Encoding categorical variables\n",
    "- Scaling/normalization\n",
    "- Train/test splitting\n",
    "- **Output:** Features ready for modeling\n",
    "\n",
    "**3. Transformation (Domain-Specific)**\n",
    "- Spatial transformations (coordinate projections)\n",
    "- Aggregations (individual â†’ household level)\n",
    "- Privacy operations (k-anonymization)\n",
    "- **Output:** Domain-specific views\n",
    "\n",
    "### Your Current GeoSTAD Structure:\n",
    "\n",
    "```python\n",
    "def load_geostad():\n",
    "    # LOADING âœ“\n",
    "    df = pyreadstat.read_sav(path)\n",
    "    df = df[selected_columns]\n",
    "    \n",
    "    # CLEANING âœ“ (still part of loading)\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.dropna(subset=['X_2025', 'Y_2025'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_for_synthesis():\n",
    "    # PREPROCESSING âœ“ (separate function!)\n",
    "    df = df[feature_columns]  # Feature selection\n",
    "    df = df.sample(n=1000)     # Sampling\n",
    "    return df\n",
    "```\n",
    "\n",
    "**This is already well-organized!** âœ…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ebd0d",
   "metadata": {},
   "source": [
    "### When to Split: Decision Tree\n",
    "\n",
    "**Keep together IF:**\n",
    "- âœ… Preprocessing is dataset-specific (your case!)\n",
    "- âœ… File < 500 lines\n",
    "- âœ… Only 1-2 preprocessing variants\n",
    "\n",
    "**Split into separate module IF:**\n",
    "- âŒ Multiple datasets share preprocessing logic\n",
    "- âŒ Need different preprocessing pipelines for same data\n",
    "- âŒ Preprocessing becomes complex (>100 lines)\n",
    "- âŒ Want to unit test preprocessing separately\n",
    "\n",
    "### Architecture Options\n",
    "\n",
    "**Option A: Keep in data module** (CURRENT - Good!)\n",
    "```\n",
    "synlab/data/\n",
    "â”œâ”€â”€ population_geostad.py\n",
    "â”‚   â”œâ”€â”€ load_geostad()        # Loading + cleaning\n",
    "â”‚   â””â”€â”€ prepare_for_synthesis()  # Preprocessing\n",
    "â”œâ”€â”€ population_rvu.py\n",
    "â”‚   â”œâ”€â”€ load_rvu()\n",
    "â”‚   â””â”€â”€ prepare_for_synthesis()\n",
    "â””â”€â”€ configs.py\n",
    "```\n",
    "**When:** Preprocessing is dataset-specific, simple, and tightly coupled to loading.\n",
    "\n",
    "**Option B: Preprocessing subfolder**\n",
    "```\n",
    "synlab/data/\n",
    "â”œâ”€â”€ loaders/\n",
    "â”‚   â”œâ”€â”€ geostad_loader.py      # Just loading\n",
    "â”‚   â””â”€â”€ rvu_loader.py\n",
    "â”œâ”€â”€ preprocessing/\n",
    "â”‚   â”œâ”€â”€ geostad_prep.py        # GeoSTAD-specific prep\n",
    "â”‚   â”œâ”€â”€ rvu_prep.py            # RVU-specific prep\n",
    "â”‚   â””â”€â”€ common.py              # Shared preprocessing\n",
    "â””â”€â”€ configs.py\n",
    "```\n",
    "**When:** Preprocessing logic is substantial (>100 lines) OR you have shared preprocessing utilities.\n",
    "\n",
    "**Option C: Separate preprocessing module at top level**\n",
    "```\n",
    "synlab/\n",
    "â”œâ”€â”€ data/\n",
    "â”‚   â””â”€â”€ loaders/               # Pure loading only\n",
    "â”œâ”€â”€ preprocessing/             # All preprocessing\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”œâ”€â”€ features.py            # Feature engineering\n",
    "â”‚   â”œâ”€â”€ encoders.py            # Categorical encoding\n",
    "â”‚   â”œâ”€â”€ scalers.py             # Normalization\n",
    "â”‚   â””â”€â”€ pipelines.py           # sklearn-style pipelines\n",
    "â”œâ”€â”€ methods/\n",
    "â””â”€â”€ evaluation/\n",
    "```\n",
    "**When:** Large project, multiple datasets, need sklearn-compatible preprocessing pipelines, heavy ML focus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f7ba53",
   "metadata": {},
   "source": [
    "### Real-World Examples from Famous Projects\n",
    "\n",
    "**1. scikit-learn** - Separate preprocessing module\n",
    "```\n",
    "sklearn/\n",
    "â”œâ”€â”€ datasets/              # Loading only\n",
    "â”œâ”€â”€ preprocessing/         # Separate top-level module!\n",
    "â”‚   â”œâ”€â”€ _encoders.py\n",
    "â”‚   â”œâ”€â”€ _data.py\n",
    "â”‚   â””â”€â”€ _label.py\n",
    "â””â”€â”€ ...\n",
    "```\n",
    "**Why:** Generic preprocessing used across many datasets, needs to be reusable.\n",
    "\n",
    "**2. pandas-profiling / ydata-profiling** - Combined\n",
    "```\n",
    "ydata_profiling/\n",
    "â””â”€â”€ model/\n",
    "    â””â”€â”€ pandas/\n",
    "        â”œâ”€â”€ reader.py         # Loading\n",
    "        â””â”€â”€ describe.py       # Loading + basic stats together\n",
    "```\n",
    "**Why:** Analysis-focused, preprocessing tightly coupled to data structure.\n",
    "\n",
    "**3. PyTorch torchvision** - Separate transforms\n",
    "```\n",
    "torchvision/\n",
    "â”œâ”€â”€ datasets/              # Loading images\n",
    "â””â”€â”€ transforms/            # Separate preprocessing module\n",
    "    â”œâ”€â”€ functional.py\n",
    "    â””â”€â”€ transforms.py\n",
    "```\n",
    "**Why:** Same preprocessing (resize, normalize) used across many datasets.\n",
    "\n",
    "**4. statsmodels** - Combined in datasets\n",
    "```\n",
    "statsmodels/\n",
    "â””â”€â”€ datasets/\n",
    "    â””â”€â”€ anes96/\n",
    "        â””â”€â”€ data.py        # Load + basic prep together\n",
    "```\n",
    "**Why:** Dataset-specific, research-focused, not many shared operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6ce619",
   "metadata": {},
   "source": [
    "### Concrete Example: When YOU Should Split\n",
    "\n",
    "Imagine after implementing RVU and TraMOD, you notice this pattern:\n",
    "\n",
    "**Current (duplicated preprocessing):**\n",
    "```python\n",
    "# data/population_geostad.py\n",
    "def prepare_for_synthesis(df, config):\n",
    "    df = df[config.feature_columns]\n",
    "    df = df.dropna()\n",
    "    df = handle_categorical(df)  # â† repeated\n",
    "    df = normalize_coordinates(df)  # â† repeated\n",
    "    return df\n",
    "\n",
    "# data/population_rvu.py\n",
    "def prepare_for_synthesis(df, config):\n",
    "    df = df[config.feature_columns]\n",
    "    df = df.dropna()\n",
    "    df = handle_categorical(df)  # â† same code!\n",
    "    df = normalize_coordinates(df)  # â† same code!\n",
    "    return df\n",
    "```\n",
    "\n",
    "**After split (reusable preprocessing):**\n",
    "```python\n",
    "# data/preprocessing/common.py\n",
    "def handle_categorical(df, categorical_cols):\n",
    "    \"\"\"Encode categorical variables consistently.\"\"\"\n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].astype('category').cat.codes\n",
    "    return df\n",
    "\n",
    "def normalize_coordinates(df, x_col='X', y_col='Y'):\n",
    "    \"\"\"Normalize coordinates to [0, 1] range.\"\"\"\n",
    "    df[x_col] = (df[x_col] - df[x_col].min()) / (df[x_col].max() - df[x_col].min())\n",
    "    df[y_col] = (df[y_col] - df[y_col].min()) / (df[y_col].max() - df[y_col].min())\n",
    "    return df\n",
    "\n",
    "# data/population_geostad.py\n",
    "from synlab.data.preprocessing.common import handle_categorical, normalize_coordinates\n",
    "\n",
    "def prepare_for_synthesis(df, config):\n",
    "    df = df[config.feature_columns]\n",
    "    df = df.dropna()\n",
    "    df = handle_categorical(df, config.categorical_cols)\n",
    "    df = normalize_coordinates(df, 'X_2025', 'Y_2025')\n",
    "    return df\n",
    "```\n",
    "\n",
    "**Trigger point:** When you copy-paste code for the 3rd time!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75131ce4",
   "metadata": {},
   "source": [
    "### Folder Placement Guidelines\n",
    "\n",
    "**Where to put preprocessing:**\n",
    "\n",
    "**1. Small project (yours now):** Keep in `data/`\n",
    "```\n",
    "synlab/data/\n",
    "â”œâ”€â”€ population_geostad.py  # All in one âœ“\n",
    "â””â”€â”€ configs.py\n",
    "```\n",
    "\n",
    "**2. Multiple datasets, shared logic:** Create `data/preprocessing/`\n",
    "```\n",
    "synlab/data/\n",
    "â”œâ”€â”€ loaders/\n",
    "â”‚   â”œâ”€â”€ geostad_loader.py\n",
    "â”‚   â””â”€â”€ rvu_loader.py\n",
    "â”œâ”€â”€ preprocessing/          # â† New subfolder\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”œâ”€â”€ common.py          # Shared utilities\n",
    "â”‚   â”œâ”€â”€ spatial.py         # Spatial transformations\n",
    "â”‚   â””â”€â”€ categorical.py     # Encoding helpers\n",
    "â””â”€â”€ configs.py\n",
    "```\n",
    "\n",
    "**3. ML/Production system:** Top-level `preprocessing/`\n",
    "```\n",
    "synlab/\n",
    "â”œâ”€â”€ data/\n",
    "â”‚   â””â”€â”€ loaders/           # Pure loading\n",
    "â”œâ”€â”€ preprocessing/         # â† Top-level module\n",
    "â”‚   â”œâ”€â”€ pipelines/         # sklearn-style pipelines\n",
    "â”‚   â”œâ”€â”€ transformers/      # Custom transformers\n",
    "â”‚   â””â”€â”€ features/          # Feature engineering\n",
    "â”œâ”€â”€ methods/\n",
    "â””â”€â”€ models/\n",
    "```\n",
    "\n",
    "**4. Research with experiments:** Keep `data/preprocessing/` OR create `transforms/`\n",
    "```\n",
    "synlab/\n",
    "â”œâ”€â”€ data/                  # Loading only\n",
    "â”œâ”€â”€ transforms/            # â† Alternative name (PyTorch-style)\n",
    "â”‚   â”œâ”€â”€ privacy.py         # DP noise addition\n",
    "â”‚   â”œâ”€â”€ aggregation.py     # Spatial aggregation\n",
    "â”‚   â””â”€â”€ sampling.py        # Sampling strategies\n",
    "â””â”€â”€ methods/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13dc2c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== YOUR CURRENT STRUCTURE (Good for now!) ===\n",
      "\n",
      "synlab/\n",
      "â”œâ”€â”€ data/\n",
      "â”‚   â”œâ”€â”€ population_geostad.py  â† Loading + preprocessing together\n",
      "â”‚   â”œâ”€â”€ configs.py\n",
      "â”‚   â””â”€â”€ population_dpmm_wine.py\n",
      "â”œâ”€â”€ utils/\n",
      "â”‚   â”œâ”€â”€ paths.py\n",
      "â”‚   â””â”€â”€ postprocessing.py     â† Generic post-synthesis\n",
      "â””â”€â”€ methods/\n",
      "\n",
      "=== FUTURE: When you add RVU/TraMOD and see duplication ===\n",
      "\n",
      "Option 1: data/preprocessing/ subfolder\n",
      "synlab/\n",
      "â”œâ”€â”€ data/\n",
      "â”‚   â”œâ”€â”€ loaders/\n",
      "â”‚   â”‚   â”œâ”€â”€ geostad_loader.py\n",
      "â”‚   â”‚   â”œâ”€â”€ rvu_loader.py\n",
      "â”‚   â”‚   â””â”€â”€ tramod_loader.py\n",
      "â”‚   â”œâ”€â”€ preprocessing/         â† New!\n",
      "â”‚   â”‚   â”œâ”€â”€ common.py          â† Shared preprocessing\n",
      "â”‚   â”‚   â”œâ”€â”€ spatial.py         â† Coordinate handling\n",
      "â”‚   â”‚   â””â”€â”€ categorical.py     â† Encoding\n",
      "â”‚   â””â”€â”€ configs.py\n",
      "â””â”€â”€ utils/\n",
      "\n",
      "Option 2: Top-level preprocessing/ (if it grows large)\n",
      "synlab/\n",
      "â”œâ”€â”€ data/                      â† Pure loading\n",
      "â”œâ”€â”€ preprocessing/             â† New top-level module!\n",
      "â”‚   â”œâ”€â”€ pipelines.py\n",
      "â”‚   â”œâ”€â”€ features.py\n",
      "â”‚   â””â”€â”€ transforms.py\n",
      "â””â”€â”€ methods/\n",
      "\n",
      "âœ“ Recommendation: Start with Option 1 when you implement RVU!\n"
     ]
    }
   ],
   "source": [
    "# Visualize your current structure vs future options\n",
    "import sys\n",
    "\n",
    "print(\"=== YOUR CURRENT STRUCTURE (Good for now!) ===\\n\")\n",
    "print(\"synlab/\")\n",
    "print(\"â”œâ”€â”€ data/\")\n",
    "print(\"â”‚   â”œâ”€â”€ population_geostad.py  â† Loading + preprocessing together\")\n",
    "print(\"â”‚   â”œâ”€â”€ configs.py\")\n",
    "print(\"â”‚   â””â”€â”€ population_dpmm_wine.py\")\n",
    "print(\"â”œâ”€â”€ utils/\")\n",
    "print(\"â”‚   â”œâ”€â”€ paths.py\")\n",
    "print(\"â”‚   â””â”€â”€ postprocessing.py     â† Generic post-synthesis\")\n",
    "print(\"â””â”€â”€ methods/\")\n",
    "print()\n",
    "\n",
    "print(\"=== FUTURE: When you add RVU/TraMOD and see duplication ===\\n\")\n",
    "print(\"Option 1: data/preprocessing/ subfolder\")\n",
    "print(\"synlab/\")\n",
    "print(\"â”œâ”€â”€ data/\")\n",
    "print(\"â”‚   â”œâ”€â”€ loaders/\")\n",
    "print(\"â”‚   â”‚   â”œâ”€â”€ geostad_loader.py\")\n",
    "print(\"â”‚   â”‚   â”œâ”€â”€ rvu_loader.py\")\n",
    "print(\"â”‚   â”‚   â””â”€â”€ tramod_loader.py\")\n",
    "print(\"â”‚   â”œâ”€â”€ preprocessing/         â† New!\")\n",
    "print(\"â”‚   â”‚   â”œâ”€â”€ common.py          â† Shared preprocessing\")\n",
    "print(\"â”‚   â”‚   â”œâ”€â”€ spatial.py         â† Coordinate handling\")\n",
    "print(\"â”‚   â”‚   â””â”€â”€ categorical.py     â† Encoding\")\n",
    "print(\"â”‚   â””â”€â”€ configs.py\")\n",
    "print(\"â””â”€â”€ utils/\")\n",
    "print()\n",
    "\n",
    "print(\"Option 2: Top-level preprocessing/ (if it grows large)\")\n",
    "print(\"synlab/\")\n",
    "print(\"â”œâ”€â”€ data/                      â† Pure loading\")\n",
    "print(\"â”œâ”€â”€ preprocessing/             â† New top-level module!\")\n",
    "print(\"â”‚   â”œâ”€â”€ pipelines.py\")\n",
    "print(\"â”‚   â”œâ”€â”€ features.py\")\n",
    "print(\"â”‚   â””â”€â”€ transforms.py\")\n",
    "print(\"â””â”€â”€ methods/\")\n",
    "print()\n",
    "\n",
    "print(\"âœ“ Recommendation: Start with Option 1 when you implement RVU!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7761a8",
   "metadata": {},
   "source": [
    "### Summary: Decision Framework\n",
    "\n",
    "**Keep loading + preprocessing together IF:**\n",
    "1. âœ… Preprocessing is dataset-specific (not reusable)\n",
    "2. âœ… Simple operations (<100 lines of preprocessing)\n",
    "3. âœ… Only one preprocessing pipeline per dataset\n",
    "4. âœ… Research/exploration phase (your current stage!)\n",
    "\n",
    "**Split into `data/preprocessing/` subfolder WHEN:**\n",
    "1. âŒ You implement RVU and copy-paste preprocessing code\n",
    "2. âŒ Multiple datasets need the same transformations (spatial normalization, categorical encoding)\n",
    "3. âŒ Preprocessing becomes >100 lines\n",
    "4. âŒ Want to unit test preprocessing separately from loading\n",
    "\n",
    "**Move to top-level `preprocessing/` module WHEN:**\n",
    "1. âŒ Building ML pipelines with sklearn-style transformers\n",
    "2. âŒ Need complex feature engineering shared across many datasets\n",
    "3. âŒ Production system with separate data engineering team\n",
    "4. âŒ >500 lines of preprocessing code\n",
    "\n",
    "### Your Action Plan:\n",
    "\n",
    "**Now:** Keep current structure âœ“\n",
    "- `population_geostad.py` has `load_geostad()` + `prepare_for_synthesis()` together\n",
    "- Simple, clear, appropriate for 1 dataset\n",
    "\n",
    "**After RVU implementation:** Watch for code duplication\n",
    "- If you copy-paste preprocessing â†’ create `data/preprocessing/common.py`\n",
    "- Move shared functions there, import from dataset modules\n",
    "\n",
    "**Much later (if needed):** Consider top-level `preprocessing/`\n",
    "- Only if preprocessing becomes its own substantial component\n",
    "- When you need sklearn-compatible pipelines\n",
    "- When preprocessing is reused across many projects\n",
    "\n",
    "**Remember:** Most research projects (statsmodels, scipy, pandas) keep loading + preprocessing together. Only split when the pain is real!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dacce5",
   "metadata": {},
   "source": [
    "## Hardcoded Values: Config vs Loading Module?\n",
    "\n",
    "### Current Hardcoded Values in `population_geostad.py`:\n",
    "\n",
    "**In `load_geostad()`:**\n",
    "```python\n",
    "df = df.dropna(subset=[\"X_2025\", \"Y_2025\"])  # â† Hardcoded column names\n",
    "```\n",
    "\n",
    "**In `_build_domain_metadata()`:**\n",
    "```python\n",
    "continuous_cols = [\"X_2025\", \"Y_2025\"]  # â† Hardcoded\n",
    "categorical_cols = [\"SN2025\", \"orgf2025\", \"fpst2025\", \"fpnr2025\", \"grk2025\"]  # â† Hardcoded\n",
    "identifier_cols = [\"orgn2025\", \"navn2025\"]  # â† Hardcoded\n",
    "```\n",
    "\n",
    "### The Question: Should these move to config?\n",
    "\n",
    "**Current in config:**\n",
    "- âœ… `feature_columns` - used for synthesis\n",
    "- âœ… `identifier_columns` - loaded but not synthesized\n",
    "- âœ… `derived_columns` - for post-synthesis reconstruction\n",
    "\n",
    "**NOT in config (hardcoded in functions):**\n",
    "- âŒ Column types (continuous, categorical, identifier) for metadata\n",
    "- âŒ Coordinate column names for filtering\n",
    "- âŒ Spatial unit column name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c14a6b",
   "metadata": {},
   "source": [
    "### Decision Framework: Config vs Hardcoded\n",
    "\n",
    "**Move to CONFIG if:**\n",
    "1. âœ… User might want to change it between runs\n",
    "2. âœ… Different experiments need different values\n",
    "3. âœ… Shared across multiple functions\n",
    "4. âœ… Defines \"what\" to do (declarative)\n",
    "\n",
    "**Keep HARDCODED in function if:**\n",
    "1. âœ… Dataset-specific constant (never changes)\n",
    "2. âœ… Implementation detail (\"how\", not \"what\")\n",
    "3. âœ… Only used in one place\n",
    "4. âœ… Domain knowledge about data structure\n",
    "\n",
    "### Analysis of Your Hardcoded Values:\n",
    "\n",
    "**1. Coordinate columns for filtering (`[\"X_2025\", \"Y_2025\"]`)**\n",
    "\n",
    "â“ **Borderline - Could go either way**\n",
    "\n",
    "Arguments FOR config:\n",
    "- Someone might want to use different coordinate fields\n",
    "- Makes filtering behavior explicit\n",
    "- Reusable if you have multiple coordinate systems\n",
    "\n",
    "Arguments AGAINST config:\n",
    "- These are GeoSTAD-specific column names (never change)\n",
    "- Tightly coupled to this specific dataset structure\n",
    "- Adding config complexity for minimal benefit\n",
    "\n",
    "**Recommendation:** Keep hardcoded (it's dataset structure knowledge)\n",
    "\n",
    "**2. Column type classifications (continuous, categorical, identifier)**\n",
    "\n",
    "âœ… **Should move to CONFIG** \n",
    "\n",
    "Why:\n",
    "- Already have `feature_columns` and `identifier_columns` in config\n",
    "- Column types are needed for metadata generation\n",
    "- Someone might want to treat a column differently\n",
    "- Would eliminate duplication with existing config attributes\n",
    "\n",
    "**Recommendation:** Derive from existing config attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d880a6",
   "metadata": {},
   "source": [
    "### Proposed Refactoring\n",
    "\n",
    "**Option 1: Add column type mapping to config (RECOMMENDED)**\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class GeoSTADConfig:\n",
    "    # ... existing fields ...\n",
    "    \n",
    "    # Add this - derive types from existing column lists\n",
    "    @property\n",
    "    def continuous_columns(self) -> List[str]:\n",
    "        \"\"\"Columns with continuous values.\"\"\"\n",
    "        return [col for col in self.feature_columns if col in [\"X_2025\", \"Y_2025\"]]\n",
    "    \n",
    "    @property\n",
    "    def categorical_columns(self) -> List[str]:\n",
    "        \"\"\"Columns with categorical values.\"\"\"\n",
    "        return [col for col in self.feature_columns if col not in self.continuous_columns]\n",
    "    \n",
    "    @property\n",
    "    def coordinate_columns(self) -> List[str]:\n",
    "        \"\"\"Coordinate columns for spatial filtering.\"\"\"\n",
    "        return [\"X_2025\", \"Y_2025\"]\n",
    "    \n",
    "    @property  \n",
    "    def spatial_unit_column(self) -> str:\n",
    "        \"\"\"Column representing spatial units (BSU/grid cells).\"\"\"\n",
    "        return \"grk2025\"\n",
    "```\n",
    "\n",
    "Then in `_build_domain_metadata()`:\n",
    "```python\n",
    "def _build_domain_metadata(df: pd.DataFrame, config: GeoSTADConfig) -> Dict[str, Any]:\n",
    "    domain = {...}\n",
    "    \n",
    "    # Use config instead of hardcoded lists\n",
    "    for col in df.columns:\n",
    "        if col in config.continuous_columns:\n",
    "            domain[\"column_types\"][col] = \"continuous\"\n",
    "        elif col in config.categorical_columns:\n",
    "            domain[\"column_types\"][col] = \"categorical\"\n",
    "        elif col in config.identifier_columns:\n",
    "            domain[\"column_types\"][col] = \"identifier\"\n",
    "    \n",
    "    # Use config for coordinate bounds\n",
    "    if all(col in df.columns for col in config.coordinate_columns):\n",
    "        x_col, y_col = config.coordinate_columns\n",
    "        domain[\"coordinate_bounds\"] = {\n",
    "            \"X_min\": float(df[x_col].min()),\n",
    "            ...\n",
    "        }\n",
    "    \n",
    "    return domain\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- âœ… Single source of truth (config)\n",
    "- âœ… No duplication\n",
    "- âœ… Easy to understand column roles\n",
    "- âœ… Properties are computed, no extra config needed from user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acea1ee",
   "metadata": {},
   "source": [
    "**Option 2: Keep current approach (SIMPLEST)**\n",
    "\n",
    "Leave hardcoded in `_build_domain_metadata()` function.\n",
    "\n",
    "**Arguments FOR keeping it:**\n",
    "- Column names are dataset structure knowledge\n",
    "- `_build_domain_metadata()` is GeoSTAD-specific helper (private function with `_` prefix)\n",
    "- Not used outside this module\n",
    "- Changing column types is rare\n",
    "- Less code, simpler to understand\n",
    "\n",
    "**Arguments AGAINST:**\n",
    "- Duplicates information already in config\n",
    "- Less flexible if you want metadata for different column subsets\n",
    "\n",
    "### My Recommendation:\n",
    "\n",
    "**Keep coordinate filtering hardcoded, but add properties to config for column types:**\n",
    "\n",
    "```python\n",
    "# In GeoSTADConfig\n",
    "@property\n",
    "def coordinate_columns(self) -> List[str]:\n",
    "    \"\"\"Spatial coordinate columns.\"\"\"\n",
    "    return [\"X_2025\", \"Y_2025\"]\n",
    "\n",
    "@property\n",
    "def continuous_columns(self) -> List[str]:\n",
    "    \"\"\"Continuous-valued columns.\"\"\"\n",
    "    return [c for c in self.feature_columns if c in self.coordinate_columns]\n",
    "\n",
    "@property\n",
    "def categorical_columns(self) -> List[str]:\n",
    "    \"\"\"Categorical columns.\"\"\"\n",
    "    return [c for c in self.feature_columns if c not in self.continuous_columns]\n",
    "```\n",
    "\n",
    "**Why this hybrid approach:**\n",
    "1. Coordinate column names still hardcoded (they're fundamental to dataset structure)\n",
    "2. But exposed via config property (easy to override if needed)\n",
    "3. Column type logic derived from config (no duplication)\n",
    "4. Simple to implement, backwards compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b108faa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CURRENT DUPLICATION ===\n",
      "\n",
      "In GeoSTADConfig:\n",
      "  feature_columns: ['SN2025', 'orgf2025', 'fpnr2025', 'grk2025', 'X_2025', 'Y_2025']\n",
      "  identifier_columns: ['orgn2025', 'navn2025']\n",
      "\n",
      "In _build_domain_metadata() (hardcoded):\n",
      "  continuous_cols = ['X_2025', 'Y_2025']\n",
      "  categorical_cols = ['SN2025', 'orgf2025', 'fpst2025', 'fpnr2025', 'grk2025']\n",
      "  identifier_cols = ['orgn2025', 'navn2025']\n",
      "\n",
      "âŒ Problem: Column lists duplicated!\n",
      "   - feature_columns overlaps with continuous + categorical\n",
      "   - identifier_columns duplicates identifier_cols\n",
      "\n",
      "âœ… Solution: Add @property methods to config that derive types\n",
      "   - continuous = features that are coordinates\n",
      "   - categorical = features that aren't coordinates\n",
      "   - identifiers = already in config!\n",
      "\n",
      "ğŸ“ This follows DRY principle (Don't Repeat Yourself)\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate the issue - current duplication\n",
    "import sys\n",
    "modules_to_delete = [k for k in sys.modules.keys() if k.startswith('synlab')]\n",
    "for mod in modules_to_delete:\n",
    "    del sys.modules[mod]\n",
    "\n",
    "from synlab.data.configs import GeoSTADConfig\n",
    "\n",
    "config = GeoSTADConfig()\n",
    "\n",
    "print(\"=== CURRENT DUPLICATION ===\\n\")\n",
    "print(\"In GeoSTADConfig:\")\n",
    "print(f\"  feature_columns: {config.feature_columns}\")\n",
    "print(f\"  identifier_columns: {config.identifier_columns}\")\n",
    "print()\n",
    "\n",
    "print(\"In _build_domain_metadata() (hardcoded):\")\n",
    "print(\"  continuous_cols = ['X_2025', 'Y_2025']\")\n",
    "print(\"  categorical_cols = ['SN2025', 'orgf2025', 'fpst2025', 'fpnr2025', 'grk2025']\")\n",
    "print(\"  identifier_cols = ['orgn2025', 'navn2025']\")\n",
    "print()\n",
    "\n",
    "print(\"âŒ Problem: Column lists duplicated!\")\n",
    "print(\"   - feature_columns overlaps with continuous + categorical\")\n",
    "print(\"   - identifier_columns duplicates identifier_cols\")\n",
    "print()\n",
    "\n",
    "print(\"âœ… Solution: Add @property methods to config that derive types\")\n",
    "print(\"   - continuous = features that are coordinates\")\n",
    "print(\"   - categorical = features that aren't coordinates\")\n",
    "print(\"   - identifiers = already in config!\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ“ This follows DRY principle (Don't Repeat Yourself)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5cd3c0",
   "metadata": {},
   "source": [
    "### Implementation Example\n",
    "\n",
    "Here's how to add properties to GeoSTADConfig:\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class GeoSTADConfig:\n",
    "    # Existing fields...\n",
    "    feature_columns: List[str] = field(default_factory=lambda: [...])\n",
    "    identifier_columns: List[str] = field(default_factory=lambda: [...])\n",
    "    derived_columns: Dict[str, str] = field(default_factory=lambda: {...})\n",
    "    \n",
    "    # NEW: Computed properties (no duplication!)\n",
    "    @property\n",
    "    def coordinate_columns(self) -> List[str]:\n",
    "        \"\"\"Spatial coordinate columns for this dataset.\"\"\"\n",
    "        return [\"X_2025\", \"Y_2025\"]\n",
    "    \n",
    "    @property\n",
    "    def continuous_columns(self) -> List[str]:\n",
    "        \"\"\"Feature columns with continuous values.\"\"\"\n",
    "        return [c for c in self.feature_columns if c in self.coordinate_columns]\n",
    "    \n",
    "    @property\n",
    "    def categorical_columns(self) -> List[str]:\n",
    "        \"\"\"Feature columns with categorical values.\"\"\"\n",
    "        return [c for c in self.feature_columns if c not in self.continuous_columns]\n",
    "    \n",
    "    @property\n",
    "    def spatial_unit_column(self) -> str:\n",
    "        \"\"\"Column name for spatial units (BSU/grid cells).\"\"\"\n",
    "        return \"grk2025\"\n",
    "```\n",
    "\n",
    "Then in `population_geostad.py`:\n",
    "```python\n",
    "def load_geostad(config: Optional[GeoSTADConfig] = None):\n",
    "    # ...\n",
    "    if config.filter_geocoded:\n",
    "        # Use config instead of hardcoded names!\n",
    "        df = df.dropna(subset=config.coordinate_columns)\n",
    "    # ...\n",
    "\n",
    "def _build_domain_metadata(df, config):\n",
    "    # Use config properties instead of hardcoded lists!\n",
    "    for col in df.columns:\n",
    "        if col in config.continuous_columns:\n",
    "            domain[\"column_types\"][col] = \"continuous\"\n",
    "        elif col in config.categorical_columns:\n",
    "            domain[\"column_types\"][col] = \"categorical\"\n",
    "        elif col in config.identifier_columns:\n",
    "            domain[\"column_types\"][col] = \"identifier\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18900c9b",
   "metadata": {},
   "source": [
    "### Summary: Config vs Hardcoded Decision\n",
    "\n",
    "**Your question was spot on!** There IS duplication that should be cleaned up.\n",
    "\n",
    "**Current issues:**\n",
    "1. âŒ Column type lists hardcoded in `_build_domain_metadata()`\n",
    "2. âŒ Same columns defined in both config and function\n",
    "3. âŒ Violates DRY (Don't Repeat Yourself)\n",
    "\n",
    "**Recommended solution:**\n",
    "\n",
    "**Add to GeoSTADConfig:**\n",
    "- `@property coordinate_columns` - Returns [\"X_2025\", \"Y_2025\"]\n",
    "- `@property continuous_columns` - Derived from features\n",
    "- `@property categorical_columns` - Derived from features\n",
    "- `@property spatial_unit_column` - Returns \"grk2025\"\n",
    "\n",
    "**Update in population_geostad.py:**\n",
    "- Use `config.coordinate_columns` instead of hardcoded list\n",
    "- Pass `config` to `_build_domain_metadata()`\n",
    "- Use config properties instead of hardcoded column lists\n",
    "\n",
    "**Benefits:**\n",
    "- âœ… Single source of truth (config)\n",
    "- âœ… No duplication\n",
    "- âœ… Easy to override for different experiments\n",
    "- âœ… Column roles explicitly documented\n",
    "- âœ… Backwards compatible (properties compute from existing fields)\n",
    "\n",
    "**When to hardcode vs config:**\n",
    "- **Config:** Anything that defines \"what\" data to use or how to process it\n",
    "- **Hardcoded:** Implementation details that never change and are dataset structure\n",
    "\n",
    "In your case, the column NAMES can stay (they're GeoSTAD structure), but expose them via config properties so they're not duplicated and can be referenced consistently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be55ac7e",
   "metadata": {},
   "source": [
    "## Testing the Refactoring\n",
    "\n",
    "Let's verify the new config properties work correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a15f4c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NEW CONFIG PROPERTIES ===\n",
      "\n",
      "1. Coordinate columns (for spatial filtering):\n",
      "   ['X_2025', 'Y_2025']\n",
      "\n",
      "2. Continuous columns (derived from features):\n",
      "   ['X_2025', 'Y_2025']\n",
      "\n",
      "3. Categorical columns (derived from features):\n",
      "   ['SN2025', 'orgf2025', 'fpnr2025', 'grk2025']\n",
      "\n",
      "4. Spatial unit column:\n",
      "   grk2025\n",
      "\n",
      "5. Identifier columns (already existed):\n",
      "   ['orgn2025', 'navn2025']\n",
      "\n",
      "âœ… All column classifications now come from config!\n",
      "âœ… No more duplication between config and functions!\n",
      "âœ… Column types are computed properties (no extra user input needed)\n"
     ]
    }
   ],
   "source": [
    "# Test the new config properties\n",
    "import sys\n",
    "modules_to_delete = [k for k in sys.modules.keys() if k.startswith('synlab')]\n",
    "for mod in modules_to_delete:\n",
    "    del sys.modules[mod]\n",
    "\n",
    "from synlab.data.configs import GeoSTADConfig\n",
    "\n",
    "config = GeoSTADConfig()\n",
    "\n",
    "print(\"=== NEW CONFIG PROPERTIES ===\\n\")\n",
    "\n",
    "print(\"1. Coordinate columns (for spatial filtering):\")\n",
    "print(f\"   {config.coordinate_columns}\")\n",
    "print()\n",
    "\n",
    "print(\"2. Continuous columns (derived from features):\")\n",
    "print(f\"   {config.continuous_columns}\")\n",
    "print()\n",
    "\n",
    "print(\"3. Categorical columns (derived from features):\")\n",
    "print(f\"   {config.categorical_columns}\")\n",
    "print()\n",
    "\n",
    "print(\"4. Spatial unit column:\")\n",
    "print(f\"   {config.spatial_unit_column}\")\n",
    "print()\n",
    "\n",
    "print(\"5. Identifier columns (already existed):\")\n",
    "print(f\"   {config.identifier_columns}\")\n",
    "print()\n",
    "\n",
    "print(\"âœ… All column classifications now come from config!\")\n",
    "print(\"âœ… No more duplication between config and functions!\")\n",
    "print(\"âœ… Column types are computed properties (no extra user input needed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e25567a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING REFACTORED LOAD FUNCTION ===\n",
      "\n",
      "Filtered to geocoded businesses: 770,568 / 2,252,561 rows\n",
      "Removed 541 duplicate rows: 770,027 rows remaining\n",
      "âœ… Loaded 770,027 records\n",
      "\n",
      "Domain metadata (generated using config properties):\n",
      "  Column types: ['SN2025', 'orgf2025', 'fpnr2025', 'grk2025', 'X_2025', 'Y_2025', 'orgn2025', 'navn2025']\n",
      "  Coordinate bounds: ['X_min', 'X_max', 'Y_min', 'Y_max']\n",
      "  Categorical info for: ['SN2025', 'orgf2025', 'fpnr2025', 'grk2025']\n",
      "  Spatial units: 13,599\n",
      "\n",
      "Column type classification from domain:\n",
      "  SN2025          â†’ categorical\n",
      "  X_2025          â†’ continuous\n",
      "  Y_2025          â†’ continuous\n",
      "  fpnr2025        â†’ categorical\n",
      "  grk2025         â†’ categorical\n",
      "  navn2025        â†’ identifier\n",
      "  orgf2025        â†’ categorical\n",
      "  orgn2025        â†’ identifier\n",
      "\n",
      "âœ… Refactoring successful - no hardcoded column lists!\n"
     ]
    }
   ],
   "source": [
    "# Test that loading works with the refactored code\n",
    "from synlab.data.population_geostad import load_geostad\n",
    "\n",
    "print(\"=== TESTING REFACTORED LOAD FUNCTION ===\\n\")\n",
    "\n",
    "config = GeoSTADConfig()\n",
    "df, domain = load_geostad(config)\n",
    "\n",
    "print(f\"âœ… Loaded {len(df):,} records\")\n",
    "print()\n",
    "\n",
    "print(\"Domain metadata (generated using config properties):\")\n",
    "print(f\"  Column types: {list(domain['column_types'].keys())}\")\n",
    "print(f\"  Coordinate bounds: {list(domain['coordinate_bounds'].keys())}\")\n",
    "print(f\"  Categorical info for: {list(domain['categorical_info'].keys())}\")\n",
    "print(f\"  Spatial units: {domain['spatial_info']['n_spatial_units']:,}\")\n",
    "print()\n",
    "\n",
    "print(\"Column type classification from domain:\")\n",
    "for col, col_type in sorted(domain['column_types'].items()):\n",
    "    print(f\"  {col:15} â†’ {col_type}\")\n",
    "print()\n",
    "\n",
    "print(\"âœ… Refactoring successful - no hardcoded column lists!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8597276",
   "metadata": {},
   "source": [
    "## âœ… Refactoring Complete!\n",
    "\n",
    "### What Changed:\n",
    "\n",
    "**1. Added to GeoSTADConfig (`configs.py`):**\n",
    "```python\n",
    "@property\n",
    "def coordinate_columns(self) -> List[str]:\n",
    "    return [\"X_2025\", \"Y_2025\"]\n",
    "\n",
    "@property\n",
    "def continuous_columns(self) -> List[str]:\n",
    "    return [col for col in self.feature_columns if col in self.coordinate_columns]\n",
    "\n",
    "@property\n",
    "def categorical_columns(self) -> List[str]:\n",
    "    return [col for col in self.feature_columns if col not in self.continuous_columns]\n",
    "\n",
    "@property\n",
    "def spatial_unit_column(self) -> str:\n",
    "    return \"grk2025\"\n",
    "```\n",
    "\n",
    "**2. Updated in `population_geostad.py`:**\n",
    "- `load_geostad()` now uses `config.coordinate_columns` instead of hardcoded `[\"X_2025\", \"Y_2025\"]`\n",
    "- `_build_domain_metadata()` now accepts `config` parameter and uses all config properties\n",
    "- All hardcoded column lists removed!\n",
    "\n",
    "### Benefits:\n",
    "- âœ… **Single source of truth** - all column definitions in config\n",
    "- âœ… **No duplication** - column types derived from existing feature_columns\n",
    "- âœ… **Easy to override** - just subclass and override properties\n",
    "- âœ… **Self-documenting** - property names clearly indicate purpose\n",
    "- âœ… **Backwards compatible** - properties compute from existing fields\n",
    "- âœ… **DRY principle** - Don't Repeat Yourself\n",
    "\n",
    "### Pattern for Future Datasets:\n",
    "When you implement RVU or TraMOD, follow the same pattern:\n",
    "1. Define `feature_columns` and `identifier_columns` in config\n",
    "2. Add properties for dataset-specific column classifications\n",
    "3. Use config properties in loader functions\n",
    "4. No hardcoded column lists in functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa9a183",
   "metadata": {},
   "source": [
    "## Wait - Hardcoding Still Exists!\n",
    "\n",
    "### Found Hardcoding in Backward Compatibility Functions:\n",
    "\n",
    "**In `create_postal_mapping()`:**\n",
    "```python\n",
    "return mappings.get(\"fpst2025\", {})  # â† Hardcoded \"fpst2025\"\n",
    "```\n",
    "\n",
    "**In `reconstruct_postal_names()`:**\n",
    "```python\n",
    "postal_code_col: str = \"fpnr2025\",   # â† Hardcoded default\n",
    "postal_name_col: str = \"fpst2025\",   # â† Hardcoded default\n",
    "```\n",
    "\n",
    "### Two Options:\n",
    "\n",
    "**Option 1: Accept it (they're deprecated)**\n",
    "- These functions are marked \"Deprecated\" \n",
    "- Only for backward compatibility with old code\n",
    "- Will eventually be removed\n",
    "- Hardcoding is acceptable in legacy code\n",
    "\n",
    "**Option 2: Fix them for consistency**\n",
    "- Use `config.derived_columns` to get column names\n",
    "- Fully consistent with new approach\n",
    "- Better code quality\n",
    "- But adds complexity to deprecated functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_synthetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
